{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc07208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Super-7 Vertex AI Extractor (Candidate Entity Version, Clean Output)\n",
    "\n",
    "Features:\n",
    "- Uses Vertex AI Gemini with Google Search grounding\n",
    "- 3-attempt retry engine with different search strategies + early stop\n",
    "- LLM extracts clean Super-7 + per-field source metadata\n",
    "- Candidate-entity layer:\n",
    "    - Uses input name + discovered names as separate candidates\n",
    "    - Runs full extraction (all strategies) for each candidate name\n",
    "    - Scores candidates (name similarity + website/address + model confidence)\n",
    "    - Chooses the best candidate as final result\n",
    "- Clean JSON output per company:\n",
    "    {\n",
    "      \"input\": {...original CSV row...},\n",
    "      \"super7\": {...final chosen values...},\n",
    "      \"sources\": {\n",
    "         \"company_name\": {\n",
    "            \"source_url\": \"...\",\n",
    "            \"source_type\": \"...\",\n",
    "            \"rationale\": \"...\"\n",
    "         },\n",
    "         ...\n",
    "      },\n",
    "      \"candidate_debug\": [\n",
    "         {\"candidate_name\": \"...\", \"score\": 0.83},\n",
    "         ...\n",
    "      ]\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "# ==================== CONFIG ====================\n",
    "\n",
    "PROJECT_ID = \"YOUR_GCP_PROJECT_ID\"  # <-- set this\n",
    "LOCATION = \"us-east4\"\n",
    "MODEL_NAME = \"gemini-2.0-flash\"\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "EARLY_STOP_THRESHOLD = 0.92  # early-stop if attempt score >= this\n",
    "\n",
    "# Expected CSV columns (you can have more; we just use these as hints)\n",
    "DEFAULT_INPUT_COLS = [\n",
    "    \"company_name\",\n",
    "    \"country\",\n",
    "    \"state_province\",\n",
    "    \"city\",\n",
    "    \"street_address\",\n",
    "    \"postal_code\",\n",
    "    \"phone_number\",\n",
    "    \"additional_info\",\n",
    "]\n",
    "\n",
    "# Super-7 fields\n",
    "SUPER7_FIELDS = [\n",
    "    \"company_name\",\n",
    "    \"trade_style_name\",\n",
    "    \"country\",\n",
    "    \"street_address\",\n",
    "    \"postal_code\",\n",
    "    \"city\",\n",
    "    \"state_province\",\n",
    "    \"website\",\n",
    "    \"phone_number\",\n",
    "]\n",
    "\n",
    "# Source-type weights for scoring\n",
    "SOURCE_TYPE_WEIGHTS = {\n",
    "    \"government_registry\": 1.0,\n",
    "    \"official_website\": 0.9,\n",
    "    \"business_directory\": 0.75,\n",
    "    \"social_media\": 0.7,\n",
    "    \"high_quality_industry_source\": 0.75,\n",
    "    \"other\": 0.6,\n",
    "}\n",
    "\n",
    "# TLD-based URL quality hints\n",
    "TLD_WEIGHTS = {\n",
    "    \".gov\": 1.0,\n",
    "    \".gov.in\": 1.0,\n",
    "    \".gov.uk\": 1.0,\n",
    "    \".edu\": 0.9,\n",
    "    \".com\": 0.85,\n",
    "    \".org\": 0.82,\n",
    "    \".net\": 0.8,\n",
    "    \".co\": 0.78,\n",
    "    \".io\": 0.75,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AttemptResult:\n",
    "    attempt_index: int\n",
    "    strategy_name: str\n",
    "    raw_object: Dict[str, Any]\n",
    "    overall_score: float\n",
    "\n",
    "\n",
    "# ==================== CLIENT ====================\n",
    "\n",
    "def init_gemini_client() -> genai.Client:\n",
    "    \"\"\"\n",
    "    Initialize Gemini client with Vertex AI.\n",
    "    Make sure you've run:\n",
    "        gcloud auth application-default login\n",
    "    and PROJECT_ID / LOCATION are set correctly.\n",
    "    \"\"\"\n",
    "    return genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "\n",
    "# ==================== STRING & NAME HELPERS ====================\n",
    "\n",
    "CORP_SUFFIXES = [\n",
    "    \"inc\", \"inc.\", \"llc\", \"l.l.c\", \"corp\", \"corp.\", \"corporation\",\n",
    "    \"ltd\", \"ltd.\", \"limited\", \"pvt\", \"pvt.\", \"pvt ltd\", \"pty\", \"pty ltd\",\n",
    "    \"gmbh\", \"ag\", \"sa\", \"s.a.\", \"bv\", \"srl\", \"s.r.l.\",\n",
    "]\n",
    "\n",
    "\n",
    "def normalize_whitespace(s: Any) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "\n",
    "\n",
    "def strip_corp_suffix(name: str) -> str:\n",
    "    tokens = name.split()\n",
    "    lowered = [t.lower().strip(\",.\") for t in tokens]\n",
    "    end = len(tokens)\n",
    "    while end > 0 and lowered[end - 1] in CORP_SUFFIXES:\n",
    "        end -= 1\n",
    "    return normalize_whitespace(\" \".join(tokens[:end]))\n",
    "\n",
    "\n",
    "def generate_name_variants(company_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate cheap but useful variants of the input name.\n",
    "    \"\"\"\n",
    "    if not company_name:\n",
    "        return []\n",
    "\n",
    "    base = normalize_whitespace(company_name)\n",
    "    no_suffix = strip_corp_suffix(base)\n",
    "\n",
    "    variants = set()\n",
    "    variants.add(base)\n",
    "    variants.add(no_suffix)\n",
    "\n",
    "    tokens = no_suffix.split()\n",
    "    if not tokens:\n",
    "        return [base]\n",
    "\n",
    "    core = tokens[0]\n",
    "\n",
    "    if len(tokens) >= 2:\n",
    "        variants.add(\" \".join(tokens[:2]))\n",
    "    if len(tokens) >= 3:\n",
    "        variants.add(\" \".join(tokens[:3]))\n",
    "\n",
    "    variants.add(core)\n",
    "    variants.add(f\"{core} llc\")\n",
    "    variants.add(f\"{core} ltd\")\n",
    "    variants.add(f\"{core} corp\")\n",
    "\n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    for v in variants:\n",
    "        v2 = normalize_whitespace(v)\n",
    "        low = v2.lower()\n",
    "        if v2 and low not in seen:\n",
    "            cleaned.append(v2)\n",
    "            seen.add(low)\n",
    "\n",
    "    return cleaned[:10]\n",
    "\n",
    "\n",
    "def normalize_name_for_similarity(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Aggressive normalization for fuzzy name similarity.\n",
    "    - lowercasing\n",
    "    - removing non-alphanumeric\n",
    "    - removing corp suffixes\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[^a-z0-9\\s]\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    tokens = name.split()\n",
    "    end = len(tokens)\n",
    "    while end > 0 and tokens[end - 1] in [s.replace(\".\", \"\") for s in CORP_SUFFIXES]:\n",
    "        end -= 1\n",
    "    return \" \".join(tokens[:end])\n",
    "\n",
    "\n",
    "def name_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple token-overlap similarity between two names: Jaccard over tokens.\n",
    "    Returns 0.0â€“1.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(a, str) or not isinstance(b, str):\n",
    "        return 0.0\n",
    "    a_norm = normalize_name_for_similarity(a)\n",
    "    b_norm = normalize_name_for_similarity(b)\n",
    "    if not a_norm or not b_norm:\n",
    "        return 0.0\n",
    "\n",
    "    set_a = set(a_norm.split())\n",
    "    set_b = set(b_norm.split())\n",
    "    if not set_a or not set_b:\n",
    "        return 0.0\n",
    "\n",
    "    inter = len(set_a & set_b)\n",
    "    union = len(set_a | set_b)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "\n",
    "# ==================== CONTEXT & STRATEGIES ====================\n",
    "\n",
    "def build_company_context(row_dict: Dict[str, Any]) -> str:\n",
    "    lines = []\n",
    "    for col in DEFAULT_INPUT_COLS:\n",
    "        v = row_dict.get(col)\n",
    "        if v is not None and str(v).strip():\n",
    "            lines.append(f\"{col}: {v}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def build_search_strategies(\n",
    "    row_dict: Dict[str, Any],\n",
    "    variants: List[str],\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Build textual search strategies (used by the LLM).\n",
    "    \"\"\"\n",
    "    company_name = row_dict.get(\"company_name\", \"\")\n",
    "    country = row_dict.get(\"country\", \"\")\n",
    "    city = row_dict.get(\"city\", \"\")\n",
    "    state = row_dict.get(\"state_province\", \"\")\n",
    "    postal = row_dict.get(\"postal_code\", \"\")\n",
    "\n",
    "    variants_str = \", \".join(f'\"{v}\"' for v in variants)\n",
    "    base_hint = f'Input company name: \"{company_name}\"'\n",
    "\n",
    "    strategies: List[Tuple[str, str]] = []\n",
    "\n",
    "    # Strategy 1: Gov + official website priority\n",
    "    s1 = f\"\"\"\n",
    "{base_hint}\n",
    "Use combinations of:\n",
    "- full & near-full names with \"{country}\" and \"{city} {state} {postal}\"\n",
    "- terms like \"business registry\", \"companies house\", \"MCA\", \"secretary of state\"\n",
    "Prioritise:\n",
    "1) government or official business registries\n",
    "2) official corporate websites\n",
    "Use variants: {variants_str}\n",
    "\"\"\"\n",
    "    strategies.append((\"gov_official_strategy\", s1.strip()))\n",
    "\n",
    "    # Strategy 2: Address/geo disambiguation\n",
    "    s2 = f\"\"\"\n",
    "{base_hint}\n",
    "Use location as a strong filter:\n",
    "- queries: \"<variant>\" \"<city>\" \"<state>\" \"<postal>\" \"<country>\"\n",
    "to disambiguate same-named entities.\n",
    "Use variants: {variants_str}\n",
    "\"\"\"\n",
    "    strategies.append((\"address_geo_strategy\", s2.strip()))\n",
    "\n",
    "    # Strategy 3: Directory + social fallback\n",
    "    s3 = f\"\"\"\n",
    "{base_hint}\n",
    "If registry/official results are unclear:\n",
    "- search business directories like LinkedIn, Bloomberg, Crunchbase, D&B, Manta, Yelp, BBB,\n",
    "  YellowPages, OpenCorporates, and industry-specific registries\n",
    "- also company pages on social media if they contain structured business info\n",
    "Use variants: {variants_str}\n",
    "\"\"\"\n",
    "    strategies.append((\"directory_fallback_strategy\", s3.strip()))\n",
    "\n",
    "    return strategies\n",
    "\n",
    "\n",
    "# ==================== PROMPT ====================\n",
    "\n",
    "def build_super7_prompt(\n",
    "    row_dict: Dict[str, Any],\n",
    "    strategy_name: str,\n",
    "    strategy_text: str,\n",
    "    variants: List[str],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Main extraction prompt for a single attempt.\n",
    "    Asks for:\n",
    "      - \"super7\": values\n",
    "      - \"sources\": per-field metadata\n",
    "      - \"overall_confidence\": 0â€“1\n",
    "    \"\"\"\n",
    "    company_context = build_company_context(row_dict)\n",
    "    variants_str = \", \".join(f'\"{v}\"' for v in variants)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a reliable company-information extractor using the Google Search tool.\n",
    "\n",
    "########################\n",
    "# COMPANY CONTEXT INPUT\n",
    "########################\n",
    "{company_context}\n",
    "\n",
    "########################\n",
    "# NAME VARIANTS FOR SEARCH\n",
    "########################\n",
    "Use these variants in Google Search queries.\n",
    "All queries must still contain the core token from the original name:\n",
    "{variants_str}\n",
    "\n",
    "########################\n",
    "# SEARCH STRATEGY ({strategy_name})\n",
    "########################\n",
    "{strategy_text}\n",
    "\n",
    "########################\n",
    "# SOURCE CLASSIFICATION\n",
    "########################\n",
    "For each field you extract, you must rely on at least one URL and:\n",
    "- classify that URL's source_type as one of:\n",
    "    \"government_registry\",\n",
    "    \"official_website\",\n",
    "    \"business_directory\",\n",
    "    \"social_media\",\n",
    "    \"high_quality_industry_source\",\n",
    "    \"other\"\n",
    "- write a short rationale explaining why that URL supports the field.\n",
    "\n",
    "########################\n",
    "# FIELDS TO EXTRACT (Super-7)\n",
    "########################\n",
    "Use the best combination of URLs to fill:\n",
    "\n",
    "- company_name        : official legal or main operating name\n",
    "- trade_style_name    : trading / doing-business-as name (if any)\n",
    "- country\n",
    "- street_address\n",
    "- postal_code\n",
    "- city\n",
    "- state_province\n",
    "- website             : full URL of official site\n",
    "- phone_number        : primary business phone\n",
    "\n",
    "If you cannot confidently determine a field, set its value to null.\n",
    "For each field above, provide:\n",
    "- source_url\n",
    "- source_type\n",
    "- rationale (short explanation).\n",
    "\n",
    "Also provide:\n",
    "- overall_confidence: a number between 0 and 1 describing your confidence\n",
    "  in the entire Super-7 object.\n",
    "\n",
    "########################\n",
    "# OUTPUT FORMAT (STRICT JSON)\n",
    "########################\n",
    "Return EXACTLY a single JSON array with ONE object:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"super7\": {{\n",
    "      \"company_name\": \"...\",\n",
    "      \"trade_style_name\": \"...\",\n",
    "      \"country\": \"...\",\n",
    "      \"street_address\": \"...\",\n",
    "      \"postal_code\": \"...\",\n",
    "      \"city\": \"...\",\n",
    "      \"state_province\": \"...\",\n",
    "      \"website\": \"...\",\n",
    "      \"phone_number\": \"...\"\n",
    "    }},\n",
    "    \"sources\": {{\n",
    "      \"company_name\": {{\n",
    "        \"source_url\": \"...\",\n",
    "        \"source_type\": \"government_registry | official_website | business_directory | social_media | high_quality_industry_source | other\",\n",
    "        \"rationale\": \"...\"\n",
    "      }},\n",
    "      \"trade_style_name\": {{\n",
    "        \"source_url\": \"...\",\n",
    "        \"source_type\": \"...\",\n",
    "        \"rationale\": \"...\"\n",
    "      }},\n",
    "      \"country\": {{\n",
    "        \"source_url\": \"...\",\n",
    "        \"source_type\": \"...\",\n",
    "        \"rationale\": \"...\"\n",
    "      }},\n",
    "      \"street_address\": {{\n",
    "        \"source_url\": \"...\",\n",
    "        \"source_type\": \"...\",\n",
    "        \"rationale\": \"...\"\n",
    "      }},\n",
    "      \"postal_code\": {{\n",
    "        \"source_url\": \"...\",\n",
    "        \"source_type\": \"...\",\n",
    "        \"rationale\": \"...\"\n",
    "      }},\n",
    "      \"city\": {{\n",
    "        \"source_url\": \"...\",\n",
    "        \"source_type\": \"...\",\n",
    "        \"rationale\": \"...\"\n",
    "      }},\n",
    "      \"state_province\": {{\n",
    "        \"source_url\": \"...\",\n",
    "        \"source_type\": \"...\",\n",
    "        \"rationale\": \"...\"\n",
    "      }},\n",
    "      \"website\": {{\n",
    "        \"source_url\": \"...\",\n",
    "        \"source_type\": \"...\",\n",
    "        \"rationale\": \"...\"\n",
    "      }},\n",
    "      \"phone_number\": {{\n",
    "        \"source_url\": \"...\",\n",
    "        \"source_type\": \"...\",\n",
    "        \"rationale\": \"...\"\n",
    "      }}\n",
    "    }},\n",
    "    \"overall_confidence\": 0.0\n",
    "  }}\n",
    "]\n",
    "\n",
    "Rules:\n",
    "- JSON ONLY. No markdown, no commentary outside JSON.\n",
    "- If a field is null, still provide a sources[field] object with a rationale like \"not found\" or \"not clearly available\".\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "# ==================== MODEL CALLS ====================\n",
    "\n",
    "def clean_json_from_text(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Try hard to isolate the JSON part from model output.\n",
    "    \"\"\"\n",
    "    if not raw_text:\n",
    "        return \"\"\n",
    "\n",
    "    text = raw_text.strip()\n",
    "    text = re.sub(r\"^```json\", \"\", text, flags=re.IGNORECASE).strip()\n",
    "    text = re.sub(r\"^```\", \"\", text).strip()\n",
    "    if \"```\" in text:\n",
    "        text = text.split(\"```\", 1)[0].strip()\n",
    "\n",
    "    idxs = [i for i in (text.find(\"[\"), text.find(\"{\")) if i != -1]\n",
    "    if idxs:\n",
    "        start = min(idxs)\n",
    "        text = text[start:].strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def call_gemini_with_grounding(\n",
    "    client: genai.Client,\n",
    "    prompt: str,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Call Gemini with Google Search grounding and robust JSON parsing.\n",
    "    \"\"\"\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=0.0,\n",
    "        tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    "    )\n",
    "\n",
    "    raw_text = \"\"\n",
    "    cleaned = \"\"\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=prompt,\n",
    "            config=config,\n",
    "        )\n",
    "        raw_text = response.text or \"\"\n",
    "        cleaned = clean_json_from_text(raw_text)\n",
    "\n",
    "        try:\n",
    "            data = json.loads(cleaned)\n",
    "        except json.JSONDecodeError:\n",
    "            # bracket-slice fallback\n",
    "            first = cleaned.find(\"[\")\n",
    "            last = cleaned.rfind(\"]\")\n",
    "            if first != -1 and last != -1 and last > first:\n",
    "                candidate = cleaned[first : last + 1]\n",
    "                data = json.loads(candidate)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        if isinstance(data, list) and data:\n",
    "            return data[0]\n",
    "        elif isinstance(data, dict):\n",
    "            return data\n",
    "        else:\n",
    "            print(\"âš  Unexpected JSON type:\", type(data))\n",
    "            return None\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"âš  JSON decode error:\", e)\n",
    "        print(\"âš  Cleaned snippet (first 600 chars):\")\n",
    "        print(cleaned[:600])\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"âš  Error in call_gemini_with_grounding:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==================== SIMPLE URL & CONFIDENCE SCORING ====================\n",
    "\n",
    "def extract_domain(url: Optional[str]) -> str:\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    m = re.search(r\"https?://([^/]+)\", url)\n",
    "    return m.group(1).lower() if m else \"\"\n",
    "\n",
    "\n",
    "def url_quality(url: Optional[str]) -> float:\n",
    "    if not url:\n",
    "        return 0.6\n",
    "    domain = extract_domain(url)\n",
    "    score = 0.7\n",
    "    for tld, w in TLD_WEIGHTS.items():\n",
    "        if domain.endswith(tld):\n",
    "            score = max(score, w)\n",
    "    if url.startswith(\"https://\"):\n",
    "        score += 0.05\n",
    "    return max(0.4, min(score, 1.0))\n",
    "\n",
    "\n",
    "def compute_overall_score(obj: Dict[str, Any]) -> float:\n",
    "    \"\"\"\n",
    "    Candidate-level score using:\n",
    "    - LLM's overall_confidence (0â€“1)\n",
    "    - source_type & URL quality for each field\n",
    "    \"\"\"\n",
    "    super7 = obj.get(\"super7\") or {}\n",
    "    sources = obj.get(\"sources\") or {}\n",
    "    base_conf = float(obj.get(\"overall_confidence\") or 0.0)\n",
    "\n",
    "    if not super7 or not sources:\n",
    "        return 0.0\n",
    "\n",
    "    per_field_scores = []\n",
    "    for field in SUPER7_FIELDS:\n",
    "        meta = sources.get(field) or {}\n",
    "        src_type = meta.get(\"source_type\", \"other\")\n",
    "        src_url = meta.get(\"source_url\")\n",
    "        src_w = SOURCE_TYPE_WEIGHTS.get(src_type, SOURCE_TYPE_WEIGHTS[\"other\"])\n",
    "        url_w = url_quality(src_url)\n",
    "\n",
    "        per_field_scores.append(base_conf * src_w * url_w)\n",
    "\n",
    "    if not per_field_scores:\n",
    "        return base_conf\n",
    "\n",
    "    return round(sum(per_field_scores) / len(per_field_scores), 4)\n",
    "\n",
    "\n",
    "# ==================== SINGLE-ENTITY RETRY ENGINE ====================\n",
    "\n",
    "def run_single_attempt(\n",
    "    client: genai.Client,\n",
    "    attempt_index: int,\n",
    "    strategy: Tuple[str, str],\n",
    "    row_dict: Dict[str, Any],\n",
    "    variants: List[str],\n",
    ") -> Optional[AttemptResult]:\n",
    "    strat_name, strat_text = strategy\n",
    "    print(f\"\\nâ–¶ Attempt {attempt_index + 1}: {strat_name}\")\n",
    "\n",
    "    prompt = build_super7_prompt(row_dict, strat_name, strat_text, variants)\n",
    "    obj = call_gemini_with_grounding(client, prompt)\n",
    "    if not obj:\n",
    "        print(\"  âš  Attempt failed (no valid JSON).\")\n",
    "        return None\n",
    "\n",
    "    overall_score = compute_overall_score(obj)\n",
    "    print(f\"  âœ” Attempt overall candidate score: {overall_score}\")\n",
    "    return AttemptResult(\n",
    "        attempt_index=attempt_index,\n",
    "        strategy_name=strat_name,\n",
    "        raw_object=obj,\n",
    "        overall_score=overall_score,\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_single_entity_for_row(\n",
    "    client: genai.Client,\n",
    "    row_dict: Dict[str, Any],\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Core extractor for a row + its current company_name.\n",
    "    Uses multi-strategy retry engine, returns the best raw LLM object or None.\n",
    "    \"\"\"\n",
    "    company_name = row_dict.get(\"company_name\", \"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"[Entity] Processing name: {company_name}\")\n",
    "\n",
    "    variants = generate_name_variants(company_name)\n",
    "    strategies = build_search_strategies(row_dict, variants)\n",
    "\n",
    "    attempts: List[AttemptResult] = []\n",
    "    print(\n",
    "        f\"=== RETRY ENGINE START (max {MAX_RETRIES} attempts, early-stop at {EARLY_STOP_THRESHOLD}) ===\"\n",
    "    )\n",
    "\n",
    "    for i in range(min(MAX_RETRIES, len(strategies))):\n",
    "        res = run_single_attempt(client, i, strategies[i], row_dict, variants)\n",
    "        if not res:\n",
    "            continue\n",
    "        attempts.append(res)\n",
    "        if res.overall_score >= EARLY_STOP_THRESHOLD:\n",
    "            print(f\"ðŸš€ Early stop at attempt {i + 1} (score {res.overall_score})\")\n",
    "            break\n",
    "\n",
    "    if not attempts:\n",
    "        print(\"âŒ All attempts failed for this name.\")\n",
    "        return None\n",
    "\n",
    "    best = max(attempts, key=lambda a: a.overall_score)\n",
    "    print(\n",
    "        f\"âœ… Best attempt for this name: #{best.attempt_index + 1} \"\n",
    "        f\"({best.strategy_name}), score={best.overall_score}\"\n",
    "    )\n",
    "    return best.raw_object\n",
    "\n",
    "\n",
    "# ==================== CANDIDATE-ENTITY LAYER ====================\n",
    "\n",
    "def build_candidate_names(\n",
    "    original_name: Any,\n",
    "    primary_obj: Optional[Dict[str, Any]],\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Build a set of candidate names to test:\n",
    "    - original input name (if string)\n",
    "    - super7.company_name from primary extraction (if string)\n",
    "    - super7.trade_style_name from primary extraction (if string)\n",
    "    plus stripped variations of each.\n",
    "    \"\"\"\n",
    "    candidates: List[str] = []\n",
    "\n",
    "    if isinstance(original_name, str) and original_name.strip():\n",
    "        candidates.append(original_name.strip())\n",
    "\n",
    "    if isinstance(primary_obj, dict):\n",
    "        super7 = primary_obj.get(\"super7\") or {}\n",
    "        cname = super7.get(\"company_name\")\n",
    "        tname = super7.get(\"trade_style_name\")\n",
    "\n",
    "        if isinstance(cname, str) and cname.strip():\n",
    "            if cname not in candidates:\n",
    "                candidates.append(cname.strip())\n",
    "        if isinstance(tname, str) and tname.strip():\n",
    "            if tname not in candidates:\n",
    "                candidates.append(tname.strip())\n",
    "\n",
    "    expanded: set = set()\n",
    "    for name in candidates:\n",
    "        if not isinstance(name, str):\n",
    "            continue\n",
    "        expanded.add(name)\n",
    "        expanded.add(strip_corp_suffix(name))\n",
    "\n",
    "    final_list: List[str] = []\n",
    "    seen = set()\n",
    "    for name in expanded:\n",
    "        if not isinstance(name, str):\n",
    "            continue\n",
    "        name_norm = normalize_whitespace(name)\n",
    "        low = name_norm.lower()\n",
    "        if name_norm and low not in seen:\n",
    "            final_list.append(name_norm)\n",
    "            seen.add(low)\n",
    "\n",
    "    return final_list[:4]\n",
    "\n",
    "\n",
    "def score_candidate_object(\n",
    "    candidate_obj: Dict[str, Any],\n",
    "    original_name: str,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Score a candidate entity using:\n",
    "    - name similarity between input and candidate.super7.company_name\n",
    "    - presence of website\n",
    "    - presence of address/postcode\n",
    "    - overall_confidence & source-based score from compute_overall_score\n",
    "    \"\"\"\n",
    "    super7 = candidate_obj.get(\"super7\") or {}\n",
    "    cand_name = super7.get(\"company_name\") or \"\"\n",
    "\n",
    "    sim = name_similarity(original_name, cand_name)\n",
    "    base_score = compute_overall_score(candidate_obj)  # includes model confidence + sources\n",
    "\n",
    "    # website / address signals\n",
    "    website = super7.get(\"website\")\n",
    "    postal = super7.get(\"postal_code\")\n",
    "    street = super7.get(\"street_address\")\n",
    "\n",
    "    website_signal = 1.0 if website else 0.0\n",
    "    address_signal = 1.0 if (postal or street) else 0.0\n",
    "\n",
    "    final = (\n",
    "        0.4 * sim +\n",
    "        0.2 * website_signal +\n",
    "        0.2 * address_signal +\n",
    "        0.2 * base_score\n",
    "    )\n",
    "    return round(final, 4)\n",
    "\n",
    "\n",
    "def process_company_row(\n",
    "    client: genai.Client,\n",
    "    row: pd.Series,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Full pipeline for one input row:\n",
    "    1) Primary extraction using the input company_name (multi-strategy, retry).\n",
    "    2) Build a set of candidate names (original + discovered).\n",
    "    3) For each candidate name, run a full extraction (all strategies) and score it.\n",
    "    4) Pick the candidate with the highest score as the final entity.\n",
    "    5) Return clean JSON: input + super7 + sources + candidate_debug.\n",
    "    \"\"\"\n",
    "    # Defensive: user sometimes passes a full DataFrame by mistake\n",
    "    if isinstance(row, pd.DataFrame):\n",
    "        raise ValueError(\n",
    "            \"process_company_row expects a single pandas Series (one row), \"\n",
    "            \"but a DataFrame was passed. Use df.iloc[i] for a specific row \"\n",
    "            \"or use process_csv_print_only() for the full file.\"\n",
    "        )\n",
    "\n",
    "    if not isinstance(row, pd.Series):\n",
    "        # Try to coerce to Series if it's a dict-like\n",
    "        row = pd.Series(dict(row))\n",
    "\n",
    "    row_dict = row.to_dict()\n",
    "    original_name = row_dict.get(\"company_name\", \"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ðŸ”¹ Input row company_name: {original_name}\")\n",
    "\n",
    "    # 1) Primary extraction on original name\n",
    "    primary_obj = extract_single_entity_for_row(client, row_dict)\n",
    "\n",
    "    # If even primary failed, bail with placeholder\n",
    "    if not primary_obj:\n",
    "        return {\n",
    "            \"input\": row_dict,\n",
    "            \"super7\": {f: None for f in SUPER7_FIELDS},\n",
    "            \"sources\": {\n",
    "                f: {\"source_url\": None, \"source_type\": \"other\", \"rationale\": \"no valid result\"}\n",
    "                for f in SUPER7_FIELDS\n",
    "            },\n",
    "            \"candidate_debug\": [],\n",
    "        }\n",
    "\n",
    "    # 2) Build candidate names\n",
    "    candidate_names = build_candidate_names(original_name, primary_obj)\n",
    "    print(\"\\n=== Candidate names to evaluate (full strategies each) ===\")\n",
    "    for cn in candidate_names:\n",
    "        print(\" -\", cn)\n",
    "\n",
    "    candidate_results: List[Tuple[str, Dict[str, Any], float]] = []\n",
    "\n",
    "    # 3) For each candidate name, run full multi-strategy extraction\n",
    "    for cand_name in candidate_names:\n",
    "        cand_row_dict = dict(row_dict)\n",
    "        cand_row_dict[\"company_name\"] = cand_name\n",
    "        cand_obj = extract_single_entity_for_row(client, cand_row_dict)\n",
    "        if not cand_obj:\n",
    "            continue\n",
    "        cand_score = score_candidate_object(cand_obj, original_name)\n",
    "        candidate_results.append((cand_name, cand_obj, cand_score))\n",
    "\n",
    "    if not candidate_results:\n",
    "        return {\n",
    "            \"input\": row_dict,\n",
    "            \"super7\": {f: None for f in SUPER7_FIELDS},\n",
    "            \"sources\": {\n",
    "                f: {\"source_url\": None, \"source_type\": \"other\", \"rationale\": \"no candidates\"}\n",
    "                for f in SUPER7_FIELDS\n",
    "            },\n",
    "            \"candidate_debug\": [],\n",
    "        }\n",
    "\n",
    "    # 4) Pick best candidate\n",
    "    candidate_results.sort(key=lambda t: t[2], reverse=True)\n",
    "    best_name, best_obj, best_score = candidate_results[0]\n",
    "    print(f\"\\nðŸŽ¯ BEST CANDIDATE: {best_name}  (score={best_score})\")\n",
    "\n",
    "    # Prepare clean super7 + sources\n",
    "    best_super7 = best_obj.get(\"super7\") or {}\n",
    "    best_sources = best_obj.get(\"sources\") or {}\n",
    "\n",
    "    clean_super7 = {f: best_super7.get(f) for f in SUPER7_FIELDS}\n",
    "    clean_sources = {}\n",
    "    for f in SUPER7_FIELDS:\n",
    "        meta = best_sources.get(f) or {}\n",
    "        clean_sources[f] = {\n",
    "            \"source_url\": meta.get(\"source_url\"),\n",
    "            \"source_type\": meta.get(\"source_type\"),\n",
    "            \"rationale\": meta.get(\"rationale\"),\n",
    "        }\n",
    "\n",
    "    candidate_debug = [\n",
    "        {\"candidate_name\": name, \"score\": sc}\n",
    "        for (name, _obj, sc) in candidate_results\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"input\": row_dict,\n",
    "        \"super7\": clean_super7,\n",
    "        \"sources\": clean_sources,\n",
    "        \"candidate_debug\": candidate_debug,\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== CSV PIPELINE (PRINT-ONLY) ====================\n",
    "\n",
    "def process_csv_print_only(input_csv_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process an input CSV and print clean JSON per row.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    client = init_gemini_client()\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    total = len(df)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"\\n\\n================= ROW {idx + 1}/{total} =================\")\n",
    "        result = process_company_row(client, row)\n",
    "        results.append(result)\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Result for row {idx} (company: {row.get('company_name', '')})\")\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ==================== CLI ENTRY (OPTIONAL) ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Super-7 Vertex AI Extractor (candidate entities, clean output, print-only).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_csv\",\n",
    "        required=True,\n",
    "        help=\"Path to input CSV with company_name, country, etc.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    process_csv_print_only(args.input_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = init_gemini_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3af384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/gampasa/.ssh/iResearch/i_research_Agent/sample1.csv\")\n",
    "row0 = df.iloc[0]   # pick one row\n",
    "\n",
    "result = process_company_row(client, row0)\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
