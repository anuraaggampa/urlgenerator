{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01682cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "super7_docling_snippet_resolver.py\n",
    "\n",
    "Pipeline:\n",
    "- Input: list of Super7Input dicts (company_name required; others optional)\n",
    "- For each company:\n",
    "    1) Tavily search to get candidate URLs (+ title + snippet)\n",
    "    2) Search-level filter (title/snippet similarity vs company name)\n",
    "    3) ScraperTool (Docling-first) to get text from HTML-like pages\n",
    "       (now explicitly skips PDFs/Word/Excel/PPT by extension)\n",
    "    4) Doc-level filter (does text even mention the company?)\n",
    "    5) Snippet extraction:\n",
    "         - windows around company name\n",
    "         - regex-based address/phone/zip candidates\n",
    "    6) LLMExtractor runs on snippets (not full doc)\n",
    "    7) Super7 summarizer:\n",
    "         - same-company guard\n",
    "         - light scoring\n",
    "         - per-field best value + provenance\n",
    "\n",
    "Additional rules:\n",
    "- News domains (e.g. thetimes-tribune.com) are blacklisted as sources\n",
    "  and are not scraped or used as primary URLs.\n",
    "- Social media + DNB + news domains are never used as Super7 value sources.\n",
    "\n",
    "Scraping ethics:\n",
    "- Realistic browser User-Agent\n",
    "- requests.Session() to reuse cookies\n",
    "- small random delays between requests\n",
    "- domain blacklist for clearly hostile/irrelevant sites\n",
    "- size guard for huge documents\n",
    "- you are responsible for only scraping sites whose ToS/robots.txt allow it\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from langchain_tavily import TavilySearch\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Setup\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "load_dotenv()\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"Please set OPENAI_API_KEY in your environment.\")\n",
    "\n",
    "if not os.getenv(\"TAVILY_API_KEY\"):\n",
    "    print(\"[WARN] TAVILY_API_KEY is not set. Tavily search will fail.\")\n",
    "\n",
    "SUPER7_FIELDS = [\n",
    "    \"company_name\",\n",
    "    \"street_address\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"country\",\n",
    "    \"zip\",\n",
    "    \"phone\",\n",
    "]\n",
    "\n",
    "# Basic corp suffixes for name normalization\n",
    "CORP_SUFFIXES = {\n",
    "    \"llc\", \"l.l.c\", \"inc\", \"inc.\", \"corp\", \"corp.\", \"corporation\",\n",
    "    \"company\", \"co\", \"co.\", \"ltd\", \"ltd.\", \"limited\", \"plc\", \"s.a.\",\n",
    "    \"gmbh\", \"oy\", \"ab\", \"bv\", \"srl\", \"sas\", \"spa\", \"holdings\", \"holding\",\n",
    "}\n",
    "\n",
    "# News outlets we want to ignore for this use-case\n",
    "NEWS_DOMAIN_BLACKLIST = {\n",
    "    \"thetimes-tribune.com\",\n",
    "    \"www.thetimes-tribune.com\",\n",
    "    # add more if they show up as bad sources\n",
    "}\n",
    "\n",
    "# Domains we DO NOT want to use for Super7 fields (social + DNB + news)\n",
    "SUMMARY_DOMAIN_EXCLUDE = {\n",
    "    \"facebook.com\",\n",
    "    \"www.facebook.com\",\n",
    "    \"instagram.com\",\n",
    "    \"www.instagram.com\",\n",
    "    \"twitter.com\",\n",
    "    \"www.twitter.com\",\n",
    "    \"x.com\",\n",
    "    \"www.x.com\",\n",
    "    \"linkedin.com\",\n",
    "    \"www.linkedin.com\",\n",
    "    \"tiktok.com\",\n",
    "    \"www.tiktok.com\",\n",
    "    \"dnb.com\",\n",
    "    \"www.dnb.com\",\n",
    "}\n",
    "SUMMARY_DOMAIN_EXCLUDE |= NEWS_DOMAIN_BLACKLIST\n",
    "\n",
    "# Domains we know are hostile / blocked / not worth scraping directly\n",
    "SCRAPER_DOMAIN_BLACKLIST = {\n",
    "    \"firesupport.uk\",\n",
    "    \"www.firesupport.uk\",\n",
    "    \"search.sunbiz.org\",\n",
    "    \"bubba.ai\",\n",
    "    \"govtribe.com\",\n",
    "    \"www.govtribe.com\",\n",
    "    \"brokersnapshot.com\",\n",
    "    \"www.brokersnapshot.com\",\n",
    "    \"dnb.com\",\n",
    "    \"www.dnb.com\",\n",
    "    \"b2bhint.com\",\n",
    "    \"www.b2bhint.com\",\n",
    "    \"yelp.com\",\n",
    "    \"www.yelp.com\",\n",
    "    \"davids-tire-shop-service.wheree.com\",\n",
    "    \"wheree.com\",\n",
    "\n",
    "    # noisy big-PDF domains from your logs\n",
    "    \"luke.af.mil\",\n",
    "    \"www.luke.af.mil\",\n",
    "    \"nrc.gov\",\n",
    "    \"www.nrc.gov\",\n",
    "}\n",
    "SCRAPER_DOMAIN_BLACKLIST |= NEWS_DOMAIN_BLACKLIST\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def get_domain(url: str) -> Optional[str]:\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        netloc = urlparse(url).netloc.lower()\n",
    "        if netloc.startswith(\"www.\"):\n",
    "            netloc = netloc[4:]\n",
    "        return netloc or None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalize_company_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize company name for similarity:\n",
    "      - lowercase\n",
    "      - & -> and\n",
    "      - remove punctuation\n",
    "      - drop typical corp suffixes\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "\n",
    "    s = name.lower()\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "\n",
    "    trans_table = str.maketrans(\"\", \"\", string.punctuation.replace(\"&\", \"\"))\n",
    "    s = s.translate(trans_table)\n",
    "\n",
    "    tokens = s.split()\n",
    "    cleaned = [t for t in tokens if t not in CORP_SUFFIXES]\n",
    "    return \" \".join(cleaned).strip()\n",
    "\n",
    "\n",
    "def jaccard_name_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Token Jaccard similarity between normalized names.\n",
    "    \"\"\"\n",
    "    na = normalize_company_name(a)\n",
    "    nb = normalize_company_name(b)\n",
    "    if not na or not nb:\n",
    "        return 0.0\n",
    "\n",
    "    set_a = set(na.split())\n",
    "    set_b = set(nb.split())\n",
    "    if not set_a or not set_b:\n",
    "        return 0.0\n",
    "\n",
    "    inter = len(set_a & set_b)\n",
    "    union = len(set_a | set_b)\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "def should_consider_search_result(\n",
    "    company_name: str,\n",
    "    title: str,\n",
    "    snippet: str,\n",
    "    min_sim: float = 0.2,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Cheap pre-filter: decide whether a Tavily result is even worth scraping.\n",
    "    - If title is somewhat similar OR snippet mentions the company name, keep.\n",
    "    - Otherwise, skip.\n",
    "    \"\"\"\n",
    "    if not title and not snippet:\n",
    "        return True  # be permissive if we know nothing\n",
    "\n",
    "    sim = jaccard_name_similarity(company_name, title or \"\")\n",
    "    if sim >= min_sim:\n",
    "        return True\n",
    "\n",
    "    if company_name and snippet:\n",
    "        if company_name.lower() in snippet.lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def doc_mentions_company(s7_name: str, text: str, min_occurrences: int = 1) -> bool:\n",
    "    \"\"\"\n",
    "    Doc-level filter: does the text even look like it's about this company?\n",
    "\n",
    "    - Check if raw company_name (lowercased) appears.\n",
    "    - If not, check main token of normalized name.\n",
    "    \"\"\"\n",
    "    if not s7_name or not text:\n",
    "        return False\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    if s7_name.lower() in text_lower:\n",
    "        return True\n",
    "\n",
    "    norm = normalize_company_name(s7_name)\n",
    "    tokens = norm.split()\n",
    "    if not tokens:\n",
    "        return False\n",
    "\n",
    "    main_token = tokens[0]\n",
    "    if not main_token:\n",
    "        return False\n",
    "\n",
    "    return text_lower.count(main_token) >= min_occurrences\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Snippet extraction\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Snippet:\n",
    "    snippet_id: int\n",
    "    snippet_type: str  # \"name_context\" | \"address_candidate\" | \"phone_candidate\" | \"zip_candidate\" | \"generic\"\n",
    "    text: str\n",
    "\n",
    "\n",
    "def extract_snippets_for_company(\n",
    "    full_text: str,\n",
    "    company_name: str,\n",
    "    max_snippets: int = 25,\n",
    "    window_chars: int = 400,\n",
    ") -> List[Snippet]:\n",
    "    \"\"\"\n",
    "    Extract a small set of high-signal snippets from the full text:\n",
    "    - windows around company-name mentions\n",
    "    - regex candidates for phone / address / zip\n",
    "    \"\"\"\n",
    "    snippets: List[Snippet] = []\n",
    "    used_spans: List[Tuple[int, int]] = []\n",
    "\n",
    "    text = full_text or \"\"\n",
    "    if not text.strip():\n",
    "        return snippets\n",
    "\n",
    "    lower_text = text.lower()\n",
    "    norm_name = normalize_company_name(company_name)\n",
    "    raw_name = company_name.lower()\n",
    "    name_variants = set()\n",
    "    if norm_name:\n",
    "        name_variants.add(norm_name)\n",
    "    if raw_name:\n",
    "        name_variants.add(raw_name)\n",
    "    if \"&\" in raw_name:\n",
    "        name_variants.add(raw_name.replace(\"&\", \"and\"))\n",
    "\n",
    "    def add_snippet(start: int, end: int, snippet_type: str):\n",
    "        nonlocal snippets, used_spans\n",
    "        # de-duplicate overlapping spans\n",
    "        for s, e in used_spans:\n",
    "            if not (end <= s or start >= e):\n",
    "                return\n",
    "        chunk = text[start:end].strip()\n",
    "        if not chunk:\n",
    "            return\n",
    "        snippet_id = len(snippets) + 1\n",
    "        snippets.append(Snippet(snippet_id=snippet_id, snippet_type=snippet_type, text=chunk))\n",
    "        used_spans.append((start, end))\n",
    "\n",
    "    # --- 1) Name-anchored snippets ---\n",
    "    for variant in name_variants:\n",
    "        if not variant:\n",
    "            continue\n",
    "        idx = 0\n",
    "        while True:\n",
    "            idx = lower_text.find(variant, idx)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            start = max(0, idx - window_chars)\n",
    "            end = min(len(text), idx + len(variant) + window_chars)\n",
    "            add_snippet(start, end, \"name_context\")\n",
    "            idx = idx + len(variant)\n",
    "            if len(snippets) >= max_snippets:\n",
    "                return snippets\n",
    "\n",
    "    # --- 2) Regex-based phone candidates ---\n",
    "    phone_pattern = re.compile(r\"\\+?\\d[\\d\\-\\s\\(\\)]{7,}\")\n",
    "    for m in phone_pattern.finditer(text):\n",
    "        start = max(0, m.start() - 80)\n",
    "        end = min(len(text), m.end() + 80)\n",
    "        add_snippet(start, end, \"phone_candidate\")\n",
    "        if len(snippets) >= max_snippets:\n",
    "            return snippets\n",
    "\n",
    "    # --- 3) Regex-based zip candidates (US-style, approximate) ---\n",
    "    zip_pattern = re.compile(r\"\\b\\d{5}(?:-\\d{4})?\\b\")\n",
    "    for m in zip_pattern.finditer(text):\n",
    "        start = max(0, m.start() - 80)\n",
    "        end = min(len(text), m.end() + 80)\n",
    "        add_snippet(start, end, \"zip_candidate\")\n",
    "        if len(snippets) >= max_snippets:\n",
    "            return snippets\n",
    "\n",
    "    # --- 4) Address-ish lines fallback (only if still few snippets) ---\n",
    "    if len(snippets) < max_snippets:\n",
    "        lines = text.splitlines()\n",
    "        address_keywords = [\n",
    "            \"street\", \"st.\", \"st \", \"road\", \"rd.\", \"rd \",\n",
    "            \"avenue\", \"ave.\", \"ave \", \"boulevard\", \"blvd\",\n",
    "            \"lane\", \"ln.\", \"ln \", \"drive\", \"dr.\", \"dr \",\n",
    "        ]\n",
    "        for line in lines:\n",
    "            l = line.lower()\n",
    "            if any(kw in l for kw in address_keywords) and any(ch.isdigit() for ch in l):\n",
    "                chunk = line.strip()\n",
    "                if not chunk:\n",
    "                    continue\n",
    "                snippet_id = len(snippets) + 1\n",
    "                snippets.append(Snippet(snippet_id=snippet_id, snippet_type=\"address_candidate\", text=chunk))\n",
    "                if len(snippets) >= max_snippets:\n",
    "                    break\n",
    "\n",
    "    # If we somehow got nothing, add a generic first N chars as a last resort\n",
    "    if not snippets:\n",
    "        chunk = text[:800].strip()\n",
    "        if chunk:\n",
    "            snippets.append(Snippet(snippet_id=1, snippet_type=\"generic\", text=chunk))\n",
    "\n",
    "    return snippets\n",
    "\n",
    "\n",
    "def snippets_to_prompt_block(snippets: List[Snippet]) -> str:\n",
    "    \"\"\"\n",
    "    Convert snippets into a textual block for the LLM prompt.\n",
    "    \"\"\"\n",
    "    if not snippets:\n",
    "        return \"No snippets were extracted; the document text was empty or uninformative.\"\n",
    "\n",
    "    lines = [\"Here are the extracted snippets (pre-filtered for likely relevance):\"]\n",
    "    for sn in snippets:\n",
    "        lines.append(f\"[SNIPPET {sn.snippet_id}] type={sn.snippet_type}\")\n",
    "        lines.append(sn.text)\n",
    "        lines.append(\"\")  # blank line between snippets\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Models\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class Super7Input(BaseModel):\n",
    "    company_name: str\n",
    "    country: Optional[str] = None\n",
    "    state: Optional[str] = None\n",
    "    city: Optional[str] = None\n",
    "    street_address: Optional[str] = None\n",
    "    zip: Optional[str] = None\n",
    "    phone: Optional[str] = None\n",
    "\n",
    "\n",
    "class ExtractedEntity(BaseModel):\n",
    "    entity_type: str\n",
    "    value: str\n",
    "    source_urls: List[str] = Field(default_factory=list)\n",
    "    confidence: Optional[float] = None  # should be in [0,1]\n",
    "\n",
    "\n",
    "class PageExtractionResult(BaseModel):\n",
    "    url: str\n",
    "    entities: List[ExtractedEntity] = Field(default_factory=list)\n",
    "    match_score_name: float = 0.0\n",
    "    match_score_address: float = 0.0\n",
    "    match_score_phone: float = 0.0\n",
    "    looks_like_official_site: bool = False\n",
    "    overall_score: float = 0.0\n",
    "    reason: str = \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CandidateRecord:\n",
    "    url: str\n",
    "    source_type: str\n",
    "    extraction: PageExtractionResult\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Web search\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class WebSearchTool:\n",
    "    def __init__(self, max_results: int = 5):\n",
    "        key = os.getenv(\"TAVILY_API_KEY\")\n",
    "        if not key:\n",
    "            raise RuntimeError(\"Missing TAVILY_API_KEY.\")\n",
    "        self.tool = TavilySearch(max_results=max_results, tavily_api_key=key)\n",
    "\n",
    "    def search(self, queries: List[str]) -> List[Dict[str, Any]]:\n",
    "        seen: Dict[str, Dict[str, Any]] = {}\n",
    "        for q in queries:\n",
    "            res = self.tool.invoke({\"query\": q})\n",
    "            for r in res.get(\"results\", []):\n",
    "                url = r.get(\"url\")\n",
    "                if not url:\n",
    "                    continue\n",
    "                if url not in seen:\n",
    "                    seen[url] = {\n",
    "                        \"url\": url,\n",
    "                        \"title\": r.get(\"title\", \"\"),\n",
    "                        \"source_type\": \"web_search\",\n",
    "                        \"content\": r.get(\"content\", \"\"),\n",
    "                    }\n",
    "        return list(seen.values())\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ScraperTool using Docling + polite crawling + size guard\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ScraperTool:\n",
    "    \"\"\"\n",
    "    Scraper that prefers Docling for rich formats (but now explicitly\n",
    "    skips PDFs/Word/Excel/PPT by extension), and falls back to HTML +\n",
    "    BeautifulSoup if Docling fails.\n",
    "\n",
    "    - Realistic browser-like User-Agent\n",
    "    - requests.Session() to persist cookies\n",
    "    - small random delays between requests (throttling)\n",
    "    - domain blacklist\n",
    "    - size guard for large documents\n",
    "\n",
    "    Returns plain text/markdown truncated to max_chars.\n",
    "    \"\"\"\n",
    "\n",
    "    SKIP_EXTENSIONS = (\n",
    "        \".pdf\",\n",
    "        \".doc\",\n",
    "        \".docx\",\n",
    "        \".xls\",\n",
    "        \".xlsx\",\n",
    "        \".ppt\",\n",
    "        \".pptx\",\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        timeout: int = 10,\n",
    "        delay_range: tuple = (1.0, 3.0),\n",
    "        use_markdown: bool = True,\n",
    "        max_content_length_bytes: int = 8_000_000,  # ~8 MB limit\n",
    "    ):\n",
    "        self.timeout = timeout\n",
    "        self.delay_range = delay_range\n",
    "        self.use_markdown = use_markdown\n",
    "        self.max_content_length_bytes = max_content_length_bytes\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "            ),\n",
    "            \"Accept\": (\n",
    "                \"text/html,application/xhtml+xml,application/xml;\"\n",
    "                \"q=0.9,image/avif,image/webp,*/*;q=0.8\"\n",
    "            ),\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        })\n",
    "\n",
    "        self.converter = DocumentConverter()\n",
    "\n",
    "    def _delay(self):\n",
    "        lo, hi = self.delay_range\n",
    "        if hi > 0:\n",
    "            time.sleep(random.uniform(lo, hi))\n",
    "\n",
    "    def _scrape_html_basic(self, html: str) -> str:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "        return \"\\n\".join(\n",
    "            line.strip()\n",
    "            for line in soup.get_text(\"\\n\").splitlines()\n",
    "            if line.strip()\n",
    "        )\n",
    "\n",
    "    def _head_too_large(self, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Lightweight HEAD to check Content-Length before we download/convert.\n",
    "        If the file is larger than max_content_length_bytes, we skip it.\n",
    "        \"\"\"\n",
    "        if not self.max_content_length_bytes:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            self._delay()\n",
    "            resp = self.session.head(url, timeout=self.timeout, allow_redirects=True)\n",
    "        except requests.RequestException:\n",
    "            # If HEAD fails, don't block; we'll let normal flow decide.\n",
    "            return False\n",
    "\n",
    "        cl = resp.headers.get(\"Content-Length\")\n",
    "        if cl is None:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            size = int(cl)\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "        if size > self.max_content_length_bytes:\n",
    "            print(f\"[SCRAPER] Skipping {url} (size {size} > {self.max_content_length_bytes} bytes).\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def fetch(self, url: str, max_chars: int = 50000) -> str:\n",
    "        domain = get_domain(url) or \"\"\n",
    "        if domain in SCRAPER_DOMAIN_BLACKLIST:\n",
    "            # known problematic or unwanted domains\n",
    "            return \"\"\n",
    "\n",
    "        # Skip PDFs / Word / Excel / PPT by extension\n",
    "        path = urlparse(url).path.lower()\n",
    "        if any(path.endswith(ext) for ext in self.SKIP_EXTENSIONS):\n",
    "            # For this project, we ignore non-HTML docs\n",
    "            return \"\"\n",
    "\n",
    "        # Quick size check first (for large docs)\n",
    "        if self._head_too_large(url):\n",
    "            return \"\"\n",
    "\n",
    "        # 1) Try Docling directly with URL (for HTML-like content)\n",
    "        try:\n",
    "            self._delay()\n",
    "            result = self.converter.convert(url)\n",
    "            doc = result.document\n",
    "\n",
    "            if self.use_markdown:\n",
    "                text = doc.export_to_markdown()\n",
    "            else:\n",
    "                text = doc.export_to_markdown()  # markdown is fine for LLM\n",
    "\n",
    "            if text:\n",
    "                if len(text) > max_chars:\n",
    "                    text = text[:max_chars]\n",
    "                return text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SCRAPER] Docling failed for {url}: {e}\")\n",
    "\n",
    "        # 2) Fallback: raw HTML\n",
    "        try:\n",
    "            self._delay()\n",
    "            resp = self.session.get(url, timeout=self.timeout, allow_redirects=True)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"[SCRAPER] Failed {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "        if resp.status_code in (401, 403, 429):\n",
    "            print(f\"[SCRAPER] HTTP {resp.status_code} for {url}, skipping.\")\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            resp.raise_for_status()\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"[SCRAPER] HTTP error {resp.status_code} for {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "        text = self._scrape_html_basic(resp.text)\n",
    "        if len(text) > max_chars:\n",
    "            text = text[:max_chars]\n",
    "        return text\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LLM Extractor (snippet-based)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class LLMExtractor:\n",
    "    def __init__(self, model: str = \"gpt-4o-mini\", temperature: float = 0.0):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def build_prompt_from_snippets(\n",
    "        self,\n",
    "        s7: Super7Input,\n",
    "        url: str,\n",
    "        snippets: List[Snippet],\n",
    "    ) -> str:\n",
    "        s7_json = json.dumps(s7.model_dump(), indent=2)\n",
    "        snippet_block = snippets_to_prompt_block(snippets)\n",
    "\n",
    "        return f\"\"\"\n",
    "You are an information extraction assistant.\n",
    "\n",
    "You are given:\n",
    "- A target company Super7 input\n",
    "- A URL\n",
    "- A small set of text snippets extracted from that URL, pre-filtered for relevance\n",
    "\n",
    "Your job is to extract entities using these exact entity_type values when applicable:\n",
    "\n",
    "Super7-related:\n",
    "- \"company_name\"\n",
    "- \"street_address\"\n",
    "- \"city\"\n",
    "- \"state\"\n",
    "- \"country\"\n",
    "- \"zip\"\n",
    "- \"phone\"\n",
    "\n",
    "Identifier-related:\n",
    "- \"dot_number\"\n",
    "- \"registration_id\"\n",
    "- \"tax_id\"\n",
    "- \"mc_number\"\n",
    "\n",
    "Other:\n",
    "- \"industry\"\n",
    "- \"email\"\n",
    "- \"website\"\n",
    "- \"social_link\"\n",
    "- \"director\"\n",
    "- \"other\"\n",
    "\n",
    "For each entity:\n",
    "- entity_type: one of the above strings\n",
    "- value: string\n",
    "- source_urls: array of URLs (MUST include \"{url}\" at minimum)\n",
    "- confidence: 0.0 to 1.0\n",
    "\n",
    "Also compute:\n",
    "- match_score_name: 0.0 to 1.0 (how well the snippets match the company name)\n",
    "- match_score_address: 0.0 to 1.0\n",
    "- match_score_phone: 0.0 to 1.0\n",
    "- looks_like_official_site: true/false (is this likely the official website / main profile?)\n",
    "- overall_score: 0.0 to 1.0 (summary of how relevant this URL is to the company)\n",
    "- reason: short explanation\n",
    "\n",
    "Important:\n",
    "- The snippets may contain other companies or entities; only extract entities that clearly belong to the target company.\n",
    "- Be conservative with confidence and scores; if unsure, use lower values.\n",
    "\n",
    "Return STRICT JSON ONLY in this shape (no extra commentary):\n",
    "\n",
    "{{\n",
    "  \"url\": \"{url}\",\n",
    "  \"entities\": [\n",
    "    {{\n",
    "      \"entity_type\": \"company_name\" | \"street_address\" | \"city\" | \"state\" | \"country\" | \"zip\" | \"phone\" |\n",
    "                      \"dot_number\" | \"registration_id\" | \"tax_id\" | \"mc_number\" |\n",
    "                      \"industry\" | \"email\" | \"website\" | \"social_link\" | \"director\" | \"other\",\n",
    "      \"value\": \"<string>\",\n",
    "      \"source_urls\": [\"<url1>\", \"<url2>\", \"...\"],\n",
    "      \"confidence\": <number between 0 and 1 or null>\n",
    "    }}\n",
    "  ],\n",
    "  \"match_score_name\": <0..1>,\n",
    "  \"match_score_address\": <0..1>,\n",
    "  \"match_score_phone\": <0..1>,\n",
    "  \"looks_like_official_site\": <true or false>,\n",
    "  \"overall_score\": <0..1>,\n",
    "  \"reason\": \"<short explanation>\"\n",
    "}}\n",
    "\n",
    "Super7 input (hints, may be null):\n",
    "{s7_json}\n",
    "\n",
    "URL: {url}\n",
    "\n",
    "{snippet_block}\n",
    "\"\"\"\n",
    "\n",
    "    def extract_from_snippets(\n",
    "        self,\n",
    "        s7: Super7Input,\n",
    "        url: str,\n",
    "        snippets: List[Snippet],\n",
    "    ) -> PageExtractionResult:\n",
    "        if not snippets:\n",
    "            return PageExtractionResult(url=url)\n",
    "\n",
    "        prompt = self.build_prompt_from_snippets(s7, url, snippets)\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        raw = resp.choices[0].message.content or \"\"\n",
    "\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except Exception:\n",
    "            start = raw.find(\"{\")\n",
    "            end = raw.rfind(\"}\")\n",
    "            if start != -1 and end != -1 and end > start:\n",
    "                try:\n",
    "                    data = json.loads(raw[start:end + 1])\n",
    "                except Exception:\n",
    "                    data = {}\n",
    "            else:\n",
    "                data = {}\n",
    "\n",
    "        url_out = str(data.get(\"url\") or url)\n",
    "        ents: List[ExtractedEntity] = []\n",
    "\n",
    "        for e in data.get(\"entities\", []):\n",
    "            raw_type = e.get(\"entity_type\")\n",
    "            entity_type = \"other\" if raw_type is None else (str(raw_type) or \"other\")\n",
    "\n",
    "            raw_value = e.get(\"value\")\n",
    "            value = \"\" if raw_value is None else str(raw_value)\n",
    "\n",
    "            raw_srcs = e.get(\"source_urls\") or []\n",
    "            srcs = [str(s) for s in raw_srcs if s]\n",
    "            if url_out not in srcs:\n",
    "                srcs.append(url_out)\n",
    "\n",
    "            raw_conf = e.get(\"confidence\")\n",
    "            if isinstance(raw_conf, (int, float)):\n",
    "                confidence = max(0.0, min(float(raw_conf), 1.0))\n",
    "            else:\n",
    "                confidence = 0.0\n",
    "\n",
    "            ents.append(\n",
    "                ExtractedEntity(\n",
    "                    entity_type=entity_type,\n",
    "                    value=value,\n",
    "                    source_urls=srcs,\n",
    "                    confidence=confidence,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return PageExtractionResult(\n",
    "            url=url_out,\n",
    "            entities=ents,\n",
    "            match_score_name=float(data.get(\"match_score_name\", 0.0)),\n",
    "            match_score_address=float(data.get(\"match_score_address\", 0.0)),\n",
    "            match_score_phone=float(data.get(\"match_score_phone\", 0.0)),\n",
    "            looks_like_official_site=bool(data.get(\"looks_like_official_site\", False)),\n",
    "            overall_score=float(data.get(\"overall_score\", 0.0)),\n",
    "            reason=str(data.get(\"reason\", \"\")),\n",
    "        )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Simple scoring + same-company guard\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def score_field_candidate(\n",
    "    field: str,\n",
    "    s7: Super7Input,\n",
    "    ent: ExtractedEntity,\n",
    "    page: PageExtractionResult,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Simple scoring for a candidate entity (raw score, not normalized).\n",
    "\n",
    "    raw_score =\n",
    "        ent_confidence\n",
    "      + 0.5 * page_overall_score\n",
    "      + 0.2 if looks_like_official_site\n",
    "      + bonus if matches Super7 hint\n",
    "    \"\"\"\n",
    "    conf = ent.confidence if isinstance(ent.confidence, (int, float)) else 0.0\n",
    "    score = conf + 0.5 * page.overall_score\n",
    "    if page.looks_like_official_site:\n",
    "        score += 0.2\n",
    "\n",
    "    hint = getattr(s7, field, None)\n",
    "    if hint:\n",
    "        h = hint.lower().strip()\n",
    "        v = ent.value.lower().strip()\n",
    "        if v == h:\n",
    "            score += 0.3\n",
    "        elif h in v or v in h:\n",
    "            score += 0.15\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def is_page_same_company(\n",
    "    s7: Super7Input,\n",
    "    page: PageExtractionResult,\n",
    "    min_sim: float = 0.6,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Minimal same-company guard:\n",
    "    - Look at extracted company_name entities on this page.\n",
    "    - Compute name similarity vs. target company_name.\n",
    "    - If any >= min_sim, treat as same company.\n",
    "    - If no company_name entities at all, we allow the page (can't decide).\n",
    "    \"\"\"\n",
    "    target = s7.company_name\n",
    "    if not target:\n",
    "        return True\n",
    "\n",
    "    sims = []\n",
    "    for e in page.entities:\n",
    "        if e.entity_type == \"company_name\" and e.value:\n",
    "            sims.append(jaccard_name_similarity(target, e.value))\n",
    "\n",
    "    if not sims:\n",
    "        # no explicit company_name extracted; don't block\n",
    "        return True\n",
    "\n",
    "    best_sim = max(sims)\n",
    "    return best_sim >= min_sim\n",
    "\n",
    "\n",
    "def summarize_super7_simple(\n",
    "    s7: Super7Input,\n",
    "    candidates: List[CandidateRecord],\n",
    ") -> Dict[str, Optional[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Collect all entities from all pages and pick best per Super7 field.\n",
    "\n",
    "    For non-company_name fields, we require the page to be \"same company\"\n",
    "    according to is_page_same_company().\n",
    "\n",
    "    The returned \"confidence\" is normalized into [0,1].\n",
    "    \"\"\"\n",
    "    summary: Dict[str, Optional[Dict[str, Any]]] = {}\n",
    "\n",
    "    # max possible raw_score ~ 2.0 (conf 1 + 0.5*1 + 0.2 + 0.3)\n",
    "    RAW_SCORE_MAX = 2.0\n",
    "\n",
    "    for field in SUPER7_FIELDS:\n",
    "        best_ent = None\n",
    "        best_page = None\n",
    "        best_raw_score = -1.0\n",
    "\n",
    "        for rec in candidates:\n",
    "            page = rec.extraction\n",
    "\n",
    "            # For non-name fields, enforce same-company guard\n",
    "            if field != \"company_name\" and not is_page_same_company(s7, page):\n",
    "                continue\n",
    "\n",
    "            for ent in page.entities:\n",
    "                if ent.entity_type != field:\n",
    "                    continue\n",
    "                if not ent.value:\n",
    "                    continue\n",
    "\n",
    "                # Skip if ALL sources are excluded domains\n",
    "                allowed_sources = []\n",
    "                for src in ent.source_urls:\n",
    "                    d = get_domain(src)\n",
    "                    if d and d in SUMMARY_DOMAIN_EXCLUDE:\n",
    "                        continue\n",
    "                    allowed_sources.append(src)\n",
    "                if not allowed_sources:\n",
    "                    continue\n",
    "\n",
    "                raw_score = score_field_candidate(field, s7, ent, page)\n",
    "                if raw_score > best_raw_score:\n",
    "                    best_raw_score = raw_score\n",
    "                    best_ent = ent\n",
    "                    best_page = page\n",
    "\n",
    "        # if no good candidate, set None\n",
    "        if not best_ent or best_raw_score < 0.3:\n",
    "            summary[field] = None\n",
    "        else:\n",
    "            # normalize raw_score into [0,1] for exposed confidence\n",
    "            norm_conf = best_raw_score / RAW_SCORE_MAX\n",
    "            norm_conf = max(0.0, min(norm_conf, 1.0))\n",
    "\n",
    "            all_sources = list({s for s in best_ent.source_urls if s})\n",
    "            primary_source = all_sources[0] if all_sources else (best_page.url if best_page else \"\")\n",
    "            summary[field] = {\n",
    "                \"value\": best_ent.value,\n",
    "                \"source\": primary_source,\n",
    "                \"confidence\": norm_conf,\n",
    "                \"all_sources\": all_sources,\n",
    "            }\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Resolver\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class Super7Resolver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        search: WebSearchTool,\n",
    "        scraper: ScraperTool,\n",
    "        extractor: LLMExtractor,\n",
    "    ):\n",
    "        self.search = search\n",
    "        self.scraper = scraper\n",
    "        self.extractor = extractor\n",
    "\n",
    "    def build_queries(self, s7: Super7Input) -> List[str]:\n",
    "        name = s7.company_name.strip()\n",
    "        parts = [name]\n",
    "        if s7.city:\n",
    "            parts.append(s7.city)\n",
    "        if s7.state:\n",
    "            parts.append(s7.state)\n",
    "        if s7.country:\n",
    "            parts.append(s7.country)\n",
    "        base = \" \".join(parts)\n",
    "\n",
    "        queries = [\n",
    "            f\"{base} official website\",\n",
    "            f\"{base} company\",\n",
    "            f\"\\\"{name}\\\"\",\n",
    "        ]\n",
    "        if s7.phone:\n",
    "            queries.append(f\"\\\"{name}\\\" \\\"{s7.phone}\\\"\")\n",
    "        return queries\n",
    "\n",
    "    def process_company(self, s7: Super7Input) -> Dict[str, Any]:\n",
    "        queries = self.build_queries(s7)\n",
    "        search_results = self.search.search(queries)\n",
    "\n",
    "        candidate_records: List[CandidateRecord] = []\n",
    "        primary_url = None\n",
    "        primary_conf = 0.0\n",
    "\n",
    "        for meta in search_results:\n",
    "            url = meta[\"url\"]\n",
    "            domain = get_domain(url) or \"\"\n",
    "            if domain in NEWS_DOMAIN_BLACKLIST:\n",
    "                # For this use-case, we ignore news outlets entirely\n",
    "                continue\n",
    "\n",
    "            title = meta.get(\"title\") or \"\"\n",
    "            snippet_text = meta.get(\"content\") or \"\"\n",
    "\n",
    "            # 1) Search-level filter\n",
    "            if not should_consider_search_result(s7.company_name, title, snippet_text):\n",
    "                continue\n",
    "\n",
    "            # 2) Scrape / convert\n",
    "            full_text = self.scraper.fetch(url)\n",
    "            if not full_text.strip():\n",
    "                # fallback to Tavily snippet if nothing else\n",
    "                if not snippet_text:\n",
    "                    continue\n",
    "                full_text = snippet_text\n",
    "\n",
    "            # 3) Doc-level filter\n",
    "            if not doc_mentions_company(s7.company_name, full_text):\n",
    "                continue\n",
    "\n",
    "            # 4) Snippet extraction\n",
    "            snippets = extract_snippets_for_company(full_text, s7.company_name)\n",
    "            if not snippets:\n",
    "                continue\n",
    "\n",
    "            # 5) LLM extraction on snippets\n",
    "            extraction = self.extractor.extract_from_snippets(s7, url, snippets)\n",
    "            candidate_records.append(\n",
    "                CandidateRecord(\n",
    "                    url=url,\n",
    "                    source_type=meta.get(\"source_type\", \"web_search\"),\n",
    "                    extraction=extraction,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if extraction.overall_score > primary_conf:\n",
    "                primary_conf = extraction.overall_score\n",
    "                primary_url = url\n",
    "\n",
    "        super7_summary = summarize_super7_simple(s7, candidate_records)\n",
    "\n",
    "        return {\n",
    "            \"input\": s7.model_dump(),\n",
    "            \"primary_url\": primary_url,\n",
    "            \"primary_confidence\": primary_conf,\n",
    "            \"candidates\": [\n",
    "                {\n",
    "                    \"url\": r.url,\n",
    "                    \"overall_score\": r.extraction.overall_score,\n",
    "                    \"reason\": r.extraction.reason,\n",
    "                }\n",
    "                for r in candidate_records\n",
    "            ],\n",
    "            \"super7_summary\": super7_summary,\n",
    "        }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Batch API\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def resolve_super7_batch(super7_payloads: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    super7_payloads: list of dicts with at least \"company_name\".\n",
    "    Returns: {\"results\": [ ... per-company dict ... ]}\n",
    "    \"\"\"\n",
    "    search = WebSearchTool(max_results=5)\n",
    "    scraper = ScraperTool(\n",
    "        timeout=10,\n",
    "        delay_range=(1.5, 4.0),\n",
    "        max_content_length_bytes=8_000_000,\n",
    "    )\n",
    "    extractor = LLMExtractor(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "    resolver = Super7Resolver(search, scraper, extractor)\n",
    "\n",
    "    results = []\n",
    "    for payload in super7_payloads:\n",
    "        s7 = Super7Input(**payload)\n",
    "        out = resolver.process_company(s7)\n",
    "        results.append(out)\n",
    "\n",
    "    return {\"results\": results}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Manual test\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_input  =  [\n",
    "        {\n",
    "            \"company_name\": \"r&k firesupport llc\"\n",
    "        },\n",
    "        {\n",
    "            \"company_name\": \"Home Fit solutions LLC\",\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"company_name\": \"David's Tireshop\",\n",
    "        },\n",
    "        {\n",
    "            \"company_name\": \"Closhare LLc\",\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"company_name\": \"Nexapoint Holding\",\n",
    "        },\n",
    "        {\n",
    "            \"company_name\": \"Making you happy logistics llc\",\n",
    "        },\n",
    "        {\n",
    "            \"company_name\": \"butler & associates construction,inc\",\n",
    "        },\n",
    "        {\n",
    "            \"company_name\": \"focus wound care centre\",\n",
    "        }\n",
    "    ]\n",
    "    res = resolve_super7_batch(batch_input)\n",
    "    print(json.dumps(res, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
