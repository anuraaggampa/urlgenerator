{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcee149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "super7_resolver_gemini_ddg.py\n",
    "\n",
    "Pipeline:\n",
    "- Input: Super7Input (company name + optional hints)\n",
    "- Web search (DuckDuckGo) to get candidate URLs\n",
    "- Scrape each URL (HTML) with polite rules\n",
    "- Extract focused snippets (name, address, phone, zip)\n",
    "- Use LLM (Gemini via LangChain) to:\n",
    "  - Extract entities (with per-entity source_urls + confidence)\n",
    "  - Compute page-level match scores\n",
    "- Score & select best candidate per Super7 field\n",
    "- Output JSON with:\n",
    "  - primary_url, primary_confidence\n",
    "  - candidates (URLs + scores)\n",
    "  - super7_summary (value, source, confidence, all_sources per field)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# NEW: Gemini + DuckDuckGo imports\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Constants / Config\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Domains we NEVER want to scrape at all (for HTML scraping).\n",
    "SCRAPER_DOMAIN_BLACKLIST = {\n",
    "    \"www.dnb.com\",\n",
    "    \"dnb.com\",\n",
    "}\n",
    "\n",
    "# Domains we don't want to use as primary sources in Super7 summary\n",
    "SUMMARY_DOMAIN_EXCLUDE = {\n",
    "    # Data vendors / noisy aggregators\n",
    "    \"www.dnb.com\",\n",
    "    \"dnb.com\",\n",
    "    \"www.b2bhint.com\",\n",
    "    \"b2bhint.com\",\n",
    "    # Social / user-generated\n",
    "    \"www.facebook.com\",\n",
    "    \"facebook.com\",\n",
    "    \"www.instagram.com\",\n",
    "    \"instagram.com\",\n",
    "    \"x.com\",\n",
    "    \"twitter.com\",\n",
    "    \"www.tiktok.com\",\n",
    "    \"tiktok.com\",\n",
    "    # News / media\n",
    "    \"www.thetimes-tribune.com\",\n",
    "    \"thetimes-tribune.com\",\n",
    "}\n",
    "\n",
    "# File extensions we skip (we don't want PDFs/Office for this stage)\n",
    "SKIP_EXTENSIONS = {\n",
    "    \".pdf\", \".doc\", \".docx\", \".xls\", \".xlsx\", \".ppt\", \".pptx\",\n",
    "    \".zip\", \".rar\",\n",
    "}\n",
    "\n",
    "# Maximum HTML text length per page (post-clean) we keep\n",
    "MAX_HTML_CHARS = 50000\n",
    "\n",
    "# Throttling boundaries (to be polite)\n",
    "REQUEST_DELAY_MIN = 0.5\n",
    "REQUEST_DELAY_MAX = 1.5\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "CORP_SUFFIXES = [\n",
    "    \"llc\", \"inc\", \"corp\", \"corporation\", \"ltd\", \"limited\",\n",
    "    \"oy\", \"oyj\", \"sa\", \"gmbh\", \"plc\", \"lp\", \"llp\", \"bv\",\n",
    "    \"srl\", \"sro\", \"pte\", \"sdn\", \"bhd\", \"ag\", \"nv\"\n",
    "]\n",
    "\n",
    "\n",
    "def normalize_company_name(name: str) -> str:\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    s = name.lower()\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    tokens = [t for t in s.split() if t]\n",
    "    filtered = [t for t in tokens if t not in CORP_SUFFIXES]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "\n",
    "def jaccard_name_similarity(a: str, b: str) -> float:\n",
    "    na = set(normalize_company_name(a).split())\n",
    "    nb = set(normalize_company_name(b).split())\n",
    "    if not na or not nb:\n",
    "        return 0.0\n",
    "    inter = len(na & nb)\n",
    "    union = len(na | nb)\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def url_has_skip_extension(url: str) -> bool:\n",
    "    path = urlparse(url).path.lower()\n",
    "    for ext in SKIP_EXTENSIONS:\n",
    "        if path.endswith(ext):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def should_consider_search_result(company_name: str, title: str, snippet: str) -> bool:\n",
    "    \"\"\"\n",
    "    Decide if a search result looks relevant enough to scrape.\n",
    "    \"\"\"\n",
    "    if not (title or snippet):\n",
    "        return False\n",
    "\n",
    "    sim = jaccard_name_similarity(company_name, title or \"\")\n",
    "    if sim >= 0.2:\n",
    "        return True\n",
    "\n",
    "    # check company name (normalized) appears in snippet\n",
    "    norm_name = normalize_company_name(company_name)\n",
    "    if norm_name and snippet:\n",
    "        if norm_name in snippet.lower():\n",
    "            return True\n",
    "\n",
    "    # fallback: raw name substring match\n",
    "    if company_name and snippet and company_name.lower() in snippet.lower():\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def doc_mentions_company(company_name: str, text: str, min_token_hits: int = 2) -> bool:\n",
    "    \"\"\"\n",
    "    Quick filter: does the doc text look like it's about this company?\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "\n",
    "    # raw name match\n",
    "    if company_name.lower() in text.lower():\n",
    "        return True\n",
    "\n",
    "    norm = normalize_company_name(company_name)\n",
    "    tokens = [t for t in norm.split() if t]\n",
    "    if not tokens:\n",
    "        return False\n",
    "\n",
    "    # pick the longest token as main\n",
    "    main = max(tokens, key=len)\n",
    "\n",
    "    hits = text.lower().count(main.lower())\n",
    "    return hits >= min_token_hits\n",
    "\n",
    "\n",
    "def extract_snippets_for_company(\n",
    "    text: str,\n",
    "    company_name: str,\n",
    "    max_snippets: int = 25,\n",
    "    window_chars: int = 300,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract small text windows where the company is mentioned + likely address/phone/zip lines.\n",
    "    Returns a list of dicts:\n",
    "      {\n",
    "        \"id\": int,\n",
    "        \"type\": \"name_context\" | \"phone_context\" | \"zip_context\" | \"address_context\" | \"generic\",\n",
    "        \"text\": str\n",
    "      }\n",
    "    \"\"\"\n",
    "    snippets: List[Dict[str, Any]] = []\n",
    "    if not text:\n",
    "        return snippets\n",
    "\n",
    "    lower_text = text.lower()\n",
    "    norm_name = normalize_company_name(company_name)\n",
    "    raw_name = company_name.lower()\n",
    "\n",
    "    variants = set()\n",
    "    if norm_name:\n",
    "        variants.add(norm_name)\n",
    "    if raw_name:\n",
    "        variants.add(raw_name)\n",
    "    # & vs and\n",
    "    variants |= {v.replace(\"&\", \" and \") for v in variants}\n",
    "    variants |= {v.replace(\" and \", \" & \") for v in variants}\n",
    "\n",
    "    # 1) name-based windows\n",
    "    used_ranges = []\n",
    "    for v in variants:\n",
    "        if not v.strip():\n",
    "            continue\n",
    "        start = 0\n",
    "        while True:\n",
    "            idx = lower_text.find(v, start)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            left = max(0, idx - window_chars)\n",
    "            right = min(len(text), idx + len(v) + window_chars)\n",
    "            candidate = text[left:right].strip()\n",
    "            if candidate:\n",
    "                snippets.append(\n",
    "                    {\n",
    "                        \"id\": len(snippets),\n",
    "                        \"type\": \"name_context\",\n",
    "                        \"text\": candidate,\n",
    "                    }\n",
    "                )\n",
    "                used_ranges.append((left, right))\n",
    "            start = idx + len(v)\n",
    "            if len(snippets) >= max_snippets:\n",
    "                break\n",
    "        if len(snippets) >= max_snippets:\n",
    "            break\n",
    "\n",
    "    if len(snippets) >= max_snippets:\n",
    "        return snippets[:max_snippets]\n",
    "\n",
    "    # 2) regex-based patterns (phone, zip, address-like lines)\n",
    "    phone_pattern = re.compile(\n",
    "        r\"(\\+?\\d[\\d\\-\\(\\)\\s]{6,}\\d)\",\n",
    "        re.MULTILINE,\n",
    "    )\n",
    "    zip_pattern = re.compile(r\"\\b\\d{5}(?:-\\d{4})?\\b\")\n",
    "    address_keywords = [\n",
    "        \"street\", \"st.\", \"st \", \"road\", \"rd.\", \"rd \",\n",
    "        \"avenue\", \"ave\", \"blvd\", \"lane\", \"ln\", \"drive\", \"dr\", \"way\",\n",
    "    ]\n",
    "\n",
    "    # We'll work line-wise\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "        lower_line = stripped.lower()\n",
    "\n",
    "        # Phone context\n",
    "        if phone_pattern.search(stripped):\n",
    "            snippets.append(\n",
    "                {\n",
    "                    \"id\": len(snippets),\n",
    "                    \"type\": \"phone_context\",\n",
    "                    \"text\": stripped,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Zip context\n",
    "        if zip_pattern.search(stripped):\n",
    "            snippets.append(\n",
    "                {\n",
    "                    \"id\": len(snippets),\n",
    "                    \"type\": \"zip_context\",\n",
    "                    \"text\": stripped,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Address-like context\n",
    "        if any(k in lower_line for k in address_keywords) and re.search(r\"\\d\", stripped):\n",
    "            snippets.append(\n",
    "                {\n",
    "                    \"id\": len(snippets),\n",
    "                    \"type\": \"address_context\",\n",
    "                    \"text\": stripped,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if len(snippets) >= max_snippets:\n",
    "            break\n",
    "\n",
    "    if not snippets:\n",
    "        # fallback: generic snippet\n",
    "        snippet = text[:800].strip()\n",
    "        if snippet:\n",
    "            snippets.append(\n",
    "                {\n",
    "                    \"id\": 0,\n",
    "                    \"type\": \"generic\",\n",
    "                    \"text\": snippet,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # deduplicate by text\n",
    "    seen_text = set()\n",
    "    unique_snippets = []\n",
    "    for sn in snippets:\n",
    "        if sn[\"text\"] not in seen_text:\n",
    "            seen_text.add(sn[\"text\"])\n",
    "            unique_snippets.append(sn)\n",
    "\n",
    "    return unique_snippets[:max_snippets]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data models\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class Super7Input(BaseModel):\n",
    "    company_name: str\n",
    "    country: Optional[str] = None\n",
    "    state: Optional[str] = None\n",
    "    city: Optional[str] = None\n",
    "    street_address: Optional[str] = None\n",
    "    zip: Optional[str] = None\n",
    "    phone: Optional[str] = None\n",
    "\n",
    "\n",
    "class ExtractedEntity(BaseModel):\n",
    "    entity_type: str\n",
    "    value: str\n",
    "    source_urls: List[str] = Field(default_factory=list)\n",
    "    confidence: Optional[float] = None\n",
    "\n",
    "\n",
    "class PageExtractionResult(BaseModel):\n",
    "    url: str\n",
    "    entities: List[ExtractedEntity] = Field(default_factory=list)\n",
    "    match_score_name: float = 0.0\n",
    "    match_score_address: float = 0.0\n",
    "    match_score_phone: float = 0.0\n",
    "    looks_like_official_site: bool = False\n",
    "    overall_score: float = 0.0\n",
    "    reason: str = \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CandidateRecord:\n",
    "    url: str\n",
    "    source_type: str\n",
    "    extraction: PageExtractionResult\n",
    "    first_seen_at: float = field(default_factory=time.time)\n",
    "    last_checked_at: float = field(default_factory=time.time)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Web search tool (DuckDuckGo)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class WebSearchTool:\n",
    "    \"\"\"\n",
    "    Simple search wrapper over DuckDuckGo (duckduckgo_search.DDGS).\n",
    "    Keeps the same contract as the Tavily-based version:\n",
    "    returns a list of dicts with keys: url, title, snippet, source_type.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_results: int = 5):\n",
    "        self.max_results = max_results\n",
    "\n",
    "    def search_candidates(self, queries: List[str]) -> List[Dict[str, Any]]:\n",
    "        seen: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "        with DDGS() as ddg:\n",
    "            for q in queries:\n",
    "                try:\n",
    "                    results = ddg.text(q, max_results=self.max_results)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"[SEARCH] DuckDuckGo search failed for query '{q}': {e}\")\n",
    "                    continue\n",
    "\n",
    "                for r in results:\n",
    "                    url = r.get(\"href\") or r.get(\"url\")\n",
    "                    title = r.get(\"title\", \"\")\n",
    "                    snippet = r.get(\"body\", \"\") or r.get(\"snippet\", \"\")\n",
    "\n",
    "                    if not url:\n",
    "                        continue\n",
    "                    if url not in seen:\n",
    "                        seen[url] = {\n",
    "                            \"url\": url,\n",
    "                            \"title\": title,\n",
    "                            \"snippet\": snippet,\n",
    "                            \"source_type\": \"duckduckgo\",\n",
    "                        }\n",
    "\n",
    "        return list(seen.values())\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Scraper tool (HTML only, no PDFs/Office)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class ScraperTool:\n",
    "    def __init__(self, timeout: int = 10):\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(\n",
    "            {\n",
    "                \"User-Agent\": (\n",
    "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                    \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "                ),\n",
    "                \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _polite_delay(self):\n",
    "        time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))\n",
    "\n",
    "    def fetch_html(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Fetch HTML page and return cleaned text.\n",
    "        No PDFs/Office – they are skipped.\n",
    "        \"\"\"\n",
    "        domain = get_domain(url)\n",
    "        if domain in SCRAPER_DOMAIN_BLACKLIST:\n",
    "            logger.info(f\"[SCRAPER] Domain blacklisted: {domain}, skipping {url}\")\n",
    "            return \"\"\n",
    "\n",
    "        if url_has_skip_extension(url):\n",
    "            logger.info(f\"[SCRAPER] Skipping non-HTML extension: {url}\")\n",
    "            return \"\"\n",
    "\n",
    "        self._polite_delay()\n",
    "\n",
    "        try:\n",
    "            resp = self.session.get(url, timeout=self.timeout)\n",
    "            resp.raise_for_status()\n",
    "        except requests.HTTPError as e:\n",
    "            status = e.response.status_code if e.response is not None else None\n",
    "            if status == 403:\n",
    "                logger.info(f\"[SCRAPER] HTTP 403 for {url}, skipping.\")\n",
    "            else:\n",
    "                logger.info(f\"[SCRAPER] HTTP error {status} for {url}: {e}\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.info(f\"[SCRAPER] Failed {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "        content_type = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        if \"text/html\" not in content_type and \"application/xhtml+xml\" not in content_type:\n",
    "            logger.info(f\"[SCRAPER] Non-HTML Content-Type ({content_type}) for {url}, skipping.\")\n",
    "            return \"\"\n",
    "\n",
    "        html = resp.text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "        lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "        text = \"\\n\".join(lines)\n",
    "\n",
    "        if len(text) > MAX_HTML_CHARS:\n",
    "            text = text[:MAX_HTML_CHARS]\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LLM extractor (Gemini)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class LLMExtractor:\n",
    "    \"\"\"\n",
    "    Uses a Gemini chat model via LangChain to:\n",
    "    - Extract entities from snippets\n",
    "    - Compute page-level match scores\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gemini-1.5-pro\", temperature: float = 0.0):\n",
    "        google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        if not google_api_key:\n",
    "            raise RuntimeError(\"Please set GOOGLE_API_KEY in your environment for Gemini.\")\n",
    "\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            # usually auto-picks API key from env, but you can pass explicitly if needed:\n",
    "            # google_api_key=google_api_key,\n",
    "            convert_system_message_to_human=True,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_float(value, default: float = 0.0) -> float:\n",
    "        \"\"\"Convert value to float, handling None and bad types gracefully.\"\"\"\n",
    "        if value is None:\n",
    "            return default\n",
    "        try:\n",
    "            return float(value)\n",
    "        except (TypeError, ValueError):\n",
    "            return default\n",
    "\n",
    "    def build_extraction_prompt(\n",
    "        self,\n",
    "        super7: Super7Input,\n",
    "        url: str,\n",
    "        snippets: List[Dict[str, Any]],\n",
    "    ) -> str:\n",
    "        s7_dict = super7.model_dump()\n",
    "        snippets_text = \"\\n\\n\".join(\n",
    "            f\"[SNIPPET {sn['id']} - {sn['type']}]\\n{sn['text']}\"\n",
    "            for sn in snippets\n",
    "        )\n",
    "\n",
    "        instructions = f\"\"\"\n",
    "You are a precise data extraction assistant.\n",
    "\n",
    "We are trying to extract the **Super7** identity fields for a company from web page snippets:\n",
    "\n",
    "Super7 fields:\n",
    "- company_name\n",
    "- street_address\n",
    "- city\n",
    "- state\n",
    "- country\n",
    "- zip\n",
    "- phone\n",
    "\n",
    "You are given:\n",
    "1) A target Super7Input (company_name is mandatory; others are optional hints).\n",
    "2) The URL of a web page.\n",
    "3) A set of focused text snippets from that page.\n",
    "\n",
    "Tasks:\n",
    "1. Decide if this page is about the SAME company as the target.\n",
    "2. Extract entities related to the company's identity:\n",
    "   - Use entity_type values exactly from this set when relevant:\n",
    "     [\"company_name\", \"street_address\", \"city\", \"state\", \"country\", \"zip\", \"phone\",\n",
    "      \"email\", \"website\", \"social_link\", \"other_id\", \"other\"]\n",
    "   - For each entity, include:\n",
    "     - value (string)\n",
    "     - source_urls: list of URLs where this value is supported (at least include the page URL)\n",
    "     - confidence: number between 0 and 1\n",
    "\n",
    "3. Compute page-level scores (0.0 to 1.0):\n",
    "   - match_score_name\n",
    "   - match_score_address\n",
    "   - match_score_phone\n",
    "   - overall_score\n",
    "   - looks_like_official_site: boolean\n",
    "   - reason: short explanation\n",
    "\n",
    "Be conservative:\n",
    "- If the page is unrelated, set scores near 0 and return few/no entities.\n",
    "- If unsure about a value, use a lower confidence.\n",
    "\n",
    "Return **STRICT JSON** only, no extra commentary, with this shape:\n",
    "\n",
    "{{\n",
    "  \"url\": \"<page URL>\",\n",
    "  \"entities\": [\n",
    "    {{\n",
    "      \"entity_type\": \"company_name\" | \"street_address\" | \"city\" | \"state\" | \"country\" | \"zip\" | \"phone\" |\n",
    "                     \"email\" | \"website\" | \"social_link\" | \"other_id\" | \"other\",\n",
    "      \"value\": \"<string>\",\n",
    "      \"source_urls\": [\"<url1>\", \"<url2>\", ...],\n",
    "      \"confidence\": <number between 0 and 1 or null>\n",
    "    }}\n",
    "  ],\n",
    "  \"match_score_name\": <number between 0 and 1>,\n",
    "  \"match_score_address\": <number between 0 and 1>,\n",
    "  \"match_score_phone\": <number between 0 and 1>,\n",
    "  \"looks_like_official_site\": <true or false>,\n",
    "  \"overall_score\": <number between 0 and 1>,\n",
    "  \"reason\": \"<short string>\"\n",
    "}}\n",
    "\n",
    "Super7Input (hints):\n",
    "\n",
    "{json.dumps(s7_dict, indent=2)}\n",
    "\n",
    "Page URL: {url}\n",
    "\n",
    "Snippets:\n",
    "{snippets_text}\n",
    "\"\"\"\n",
    "        return instructions\n",
    "\n",
    "    def extract_from_snippets(\n",
    "        self,\n",
    "        super7: Super7Input,\n",
    "        url: str,\n",
    "        snippets: List[Dict[str, Any]],\n",
    "    ) -> PageExtractionResult:\n",
    "        \"\"\"\n",
    "        Run the LLM on snippets for a single page.\n",
    "        \"\"\"\n",
    "        if not snippets:\n",
    "            return PageExtractionResult(\n",
    "                url=url,\n",
    "                entities=[],\n",
    "                match_score_name=0.0,\n",
    "                match_score_address=0.0,\n",
    "                match_score_phone=0.0,\n",
    "                looks_like_official_site=False,\n",
    "                overall_score=0.0,\n",
    "                reason=\"No snippets extracted.\",\n",
    "            )\n",
    "\n",
    "        prompt = self.build_extraction_prompt(super7, url, snippets)\n",
    "        response = self.llm.invoke(prompt)\n",
    "        text = response.content\n",
    "\n",
    "        # Try to parse JSON\n",
    "        try:\n",
    "            data = json.loads(text)\n",
    "        except Exception:\n",
    "            start = text.find(\"{\")\n",
    "            end = text.rfind(\"}\")\n",
    "            if start != -1 and end != -1 and end > start:\n",
    "                try:\n",
    "                    data = json.loads(text[start:end + 1])\n",
    "                except Exception:\n",
    "                    data = {}\n",
    "            else:\n",
    "                data = {}\n",
    "\n",
    "        if not isinstance(data, dict):\n",
    "            data = {}\n",
    "\n",
    "        url_out = data.get(\"url\", url)\n",
    "\n",
    "        entities_raw = data.get(\"entities\", [])\n",
    "        if not isinstance(entities_raw, list):\n",
    "            entities_raw = []\n",
    "\n",
    "        entities: List[ExtractedEntity] = []\n",
    "        for e in entities_raw:\n",
    "            if not isinstance(e, dict):\n",
    "                continue\n",
    "            srcs = e.get(\"source_urls\") or [url_out]\n",
    "            if not isinstance(srcs, list):\n",
    "                srcs = [url_out]\n",
    "            if url_out not in srcs:\n",
    "                srcs.append(url_out)\n",
    "\n",
    "            raw_conf = e.get(\"confidence\")\n",
    "            conf = None\n",
    "            if raw_conf is not None:\n",
    "                try:\n",
    "                    conf = float(raw_conf)\n",
    "                    # clamp to [0, 1]\n",
    "                    conf = max(0.0, min(1.0, conf))\n",
    "                except (TypeError, ValueError):\n",
    "                    conf = None\n",
    "\n",
    "            entities.append(\n",
    "                ExtractedEntity(\n",
    "                    entity_type=str(e.get(\"entity_type\", \"unknown\")),\n",
    "                    value=str(e.get(\"value\") or \"\"),\n",
    "                    source_urls=srcs,\n",
    "                    confidence=conf,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        pe = PageExtractionResult(\n",
    "            url=url_out,\n",
    "            entities=entities,\n",
    "            match_score_name=self._safe_float(data.get(\"match_score_name\"), 0.0),\n",
    "            match_score_address=self._safe_float(data.get(\"match_score_address\"), 0.0),\n",
    "            match_score_phone=self._safe_float(data.get(\"match_score_phone\"), 0.0),\n",
    "            looks_like_official_site=bool(data.get(\"looks_like_official_site\", False)),\n",
    "            overall_score=self._safe_float(data.get(\"overall_score\"), 0.0),\n",
    "            reason=str(data.get(\"reason\", \"\")),\n",
    "        )\n",
    "\n",
    "        # clamp page scores as well\n",
    "        pe.match_score_name = max(0.0, min(1.0, pe.match_score_name))\n",
    "        pe.match_score_address = max(0.0, min(1.0, pe.match_score_address))\n",
    "        pe.match_score_phone = max(0.0, min(1.0, pe.match_score_phone))\n",
    "        pe.overall_score = max(0.0, min(1.0, pe.overall_score))\n",
    "\n",
    "        return pe\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Same-company guard & scoring\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def is_page_same_company(\n",
    "    target_company_name: str,\n",
    "    page_entities: List[ExtractedEntity],\n",
    "    threshold: float = 0.6,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Decide if this page is about the same company based on extracted company_name entities.\n",
    "    \"\"\"\n",
    "    best_sim = 0.0\n",
    "    for ent in page_entities:\n",
    "        if ent.entity_type != \"company_name\":\n",
    "            continue\n",
    "        sim = jaccard_name_similarity(target_company_name, ent.value)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "    return best_sim >= threshold\n",
    "\n",
    "\n",
    "def score_field_candidate(\n",
    "    s7: Super7Input,\n",
    "    field: str,\n",
    "    ent: ExtractedEntity,\n",
    "    page: PageExtractionResult,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute a raw score for one candidate entity for one Super7 field.\n",
    "    \"\"\"\n",
    "    base_conf = ent.confidence if ent.confidence is not None else 0.0\n",
    "    score = base_conf\n",
    "\n",
    "    # page relevance\n",
    "    score += 0.5 * page.overall_score\n",
    "\n",
    "    # official site bonus\n",
    "    if page.looks_like_official_site:\n",
    "        score += 0.2\n",
    "\n",
    "    # hint-based bonus\n",
    "    hint_value = getattr(s7, field, None)\n",
    "    if hint_value and ent.value:\n",
    "        hv = str(hint_value).lower()\n",
    "        ev = ent.value.lower()\n",
    "        if hv == ev:\n",
    "            score += 0.3\n",
    "        elif hv in ev or ev in hv:\n",
    "            score += 0.15\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Super7 summarization\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def summarize_super7_simple(\n",
    "    s7: Super7Input,\n",
    "    candidates: List[CandidateRecord],\n",
    ") -> Dict[str, Optional[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    For each Super7 field, pick the best entity across all candidate pages.\n",
    "    \"\"\"\n",
    "    fields = [\n",
    "        \"company_name\",\n",
    "        \"street_address\",\n",
    "        \"city\",\n",
    "        \"state\",\n",
    "        \"country\",\n",
    "        \"zip\",\n",
    "        \"phone\",\n",
    "    ]\n",
    "\n",
    "    summary: Dict[str, Optional[Dict[str, Any]]] = {f: None for f in fields}\n",
    "\n",
    "    for field in fields:\n",
    "        best_score = -1.0\n",
    "        best_ent: Optional[ExtractedEntity] = None\n",
    "        best_sources: List[str] = []\n",
    "        best_page: Optional[PageExtractionResult] = None\n",
    "\n",
    "        for cand in candidates:\n",
    "            page = cand.extraction\n",
    "\n",
    "            # For non-name fields, enforce same-company guard\n",
    "            if field != \"company_name\":\n",
    "                if not is_page_same_company(s7.company_name, page.entities):\n",
    "                    continue\n",
    "\n",
    "            for ent in page.entities:\n",
    "                if ent.entity_type != field:\n",
    "                    continue\n",
    "                if not ent.value:\n",
    "                    continue\n",
    "\n",
    "                # filter out entities where ALL sources are excluded domains\n",
    "                all_srcs = ent.source_urls or [page.url]\n",
    "                if all(\n",
    "                    get_domain(src) in SUMMARY_DOMAIN_EXCLUDE\n",
    "                    for src in all_srcs\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                raw_score = score_field_candidate(s7, field, ent, page)\n",
    "                if raw_score > best_score:\n",
    "                    best_score = raw_score\n",
    "                    best_ent = ent\n",
    "                    best_sources = list(set(all_srcs))\n",
    "                    best_page = page\n",
    "\n",
    "        if best_ent is not None and best_score >= 0.3:\n",
    "            # Normalize raw_score ~ [0,2] → [0,1]\n",
    "            conf = min(max(best_score / 2.0, 0.0), 1.0)\n",
    "            # pick a primary source not excluded if possible\n",
    "            primary_source = None\n",
    "            for src in best_sources:\n",
    "                if get_domain(src) not in SUMMARY_DOMAIN_EXCLUDE:\n",
    "                    primary_source = src\n",
    "                    break\n",
    "            if primary_source is None and best_sources:\n",
    "                primary_source = best_sources[0]\n",
    "\n",
    "            summary[field] = {\n",
    "                \"value\": best_ent.value,\n",
    "                \"source\": primary_source,\n",
    "                \"confidence\": conf,\n",
    "                \"all_sources\": best_sources,\n",
    "            }\n",
    "        else:\n",
    "            summary[field] = None\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Resolver Orchestrator\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class Super7Resolver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        search_tool: WebSearchTool,\n",
    "        scraper: ScraperTool,\n",
    "        extractor: LLMExtractor,\n",
    "    ):\n",
    "        self.search_tool = search_tool\n",
    "        self.scraper = scraper\n",
    "        self.extractor = extractor\n",
    "\n",
    "    def build_queries(self, s7: Super7Input) -> List[str]:\n",
    "        name = s7.company_name.strip()\n",
    "        parts = [name]\n",
    "        if s7.city:\n",
    "            parts.append(s7.city)\n",
    "        if s7.state:\n",
    "            parts.append(s7.state)\n",
    "        if s7.country:\n",
    "            parts.append(s7.country)\n",
    "\n",
    "        base = \" \".join(parts)\n",
    "\n",
    "        queries = [\n",
    "            f\"{base} official website\",\n",
    "            f\"{base} company\",\n",
    "            f\"\\\"{name}\\\"\",\n",
    "        ]\n",
    "\n",
    "        if s7.phone:\n",
    "            queries.append(f\"\\\"{name}\\\" \\\"{s7.phone}\\\"\")\n",
    "\n",
    "        return queries\n",
    "\n",
    "    def process_company(self, s7: Super7Input) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Full pipeline for one company.\n",
    "        \"\"\"\n",
    "        queries = self.build_queries(s7)\n",
    "        search_results = self.search_tool.search_candidates(queries)\n",
    "\n",
    "        candidate_records: List[CandidateRecord] = []\n",
    "\n",
    "        primary_url: Optional[str] = None\n",
    "        primary_conf: float = 0.0\n",
    "\n",
    "        for sr in search_results:\n",
    "            url = sr[\"url\"]\n",
    "            title = sr.get(\"title\", \"\")\n",
    "            snippet = sr.get(\"snippet\", \"\")\n",
    "\n",
    "            if not should_consider_search_result(s7.company_name, title, snippet):\n",
    "                continue\n",
    "\n",
    "            # fetch HTML\n",
    "            text = self.scraper.fetch_html(url)\n",
    "            if not text:\n",
    "                # fall back to snippet if present\n",
    "                if snippet:\n",
    "                    text = snippet\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if not doc_mentions_company(s7.company_name, text):\n",
    "                continue\n",
    "\n",
    "            snippets = extract_snippets_for_company(text, s7.company_name)\n",
    "            if not snippets:\n",
    "                continue\n",
    "\n",
    "            extraction = self.extractor.extract_from_snippets(s7, url, snippets)\n",
    "\n",
    "            candidate_records.append(\n",
    "                CandidateRecord(\n",
    "                    url=url,\n",
    "                    source_type=sr.get(\"source_type\", \"web_search\"),\n",
    "                    extraction=extraction,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if extraction.overall_score > primary_conf:\n",
    "                primary_conf = extraction.overall_score\n",
    "                primary_url = url\n",
    "\n",
    "        # Summarize Super7 fields\n",
    "        super7_summary = summarize_super7_simple(s7, candidate_records)\n",
    "\n",
    "        # compress candidate info for output\n",
    "        candidates_out = [\n",
    "            {\n",
    "                \"url\": c.url,\n",
    "                \"overall_score\": c.extraction.overall_score,\n",
    "                \"reason\": c.extraction.reason,\n",
    "            }\n",
    "            for c in sorted(\n",
    "                candidate_records,\n",
    "                key=lambda x: x.extraction.overall_score,\n",
    "                reverse=True,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"company_id\": normalize_company_name(s7.company_name),\n",
    "            \"input\": s7.model_dump(),\n",
    "            \"primary_url\": primary_url,\n",
    "            \"primary_confidence\": primary_conf,\n",
    "            \"candidates\": candidates_out,\n",
    "            \"super7_summary\": super7_summary,\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Batch interface\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def resolve_super7_batch(super7_payloads: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    High-level function you call from your notebook / app.\n",
    "\n",
    "    Example:\n",
    "        batch_input = [\n",
    "            {\"company_name\": \"Home Fit Solutions LLC\", \"country\": \"United States\", \"city\": \"Honesdale\"},\n",
    "            {\"company_name\": \"r&k firesupport llc\"},\n",
    "        ]\n",
    "        res = resolve_super7_batch(batch_input)\n",
    "    \"\"\"\n",
    "    if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "        raise RuntimeError(\"Please set GOOGLE_API_KEY in your environment for Gemini.\")\n",
    "\n",
    "    search = WebSearchTool(max_results=5)\n",
    "    scraper = ScraperTool(timeout=10)\n",
    "    extractor = LLMExtractor(model_name=\"gemini-1.5-pro\", temperature=0.0)\n",
    "    resolver = Super7Resolver(search, scraper, extractor)\n",
    "\n",
    "    results = []\n",
    "    for payload in super7_payloads:\n",
    "        s7 = Super7Input(**payload)\n",
    "        out = resolver.process_company(s7)\n",
    "        results.append(out)\n",
    "\n",
    "    return {\"results\": results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d464cf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anuraag\\AppData\\Local\\Temp\\ipykernel_21812\\3217896819.py:386: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddg:\n",
      "2025-11-17 19:20:52,297 - INFO - response: https://www.bing.com/search?q=2ClickFit%2C+Inc.+official+website 200\n",
      "2025-11-17 19:20:53,314 - INFO - response: https://www.bing.com/search?q=2ClickFit%2C+Inc.+company 200\n",
      "2025-11-17 19:20:54,296 - INFO - response: https://www.bing.com/search?q=%222ClickFit%2C+Inc.%22 200\n",
      "C:\\Users\\anuraag\\AppData\\Local\\Temp\\ipykernel_21812\\3217896819.py:386: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddg:\n",
      "2025-11-17 19:20:54,559 - INFO - response: https://www.bing.com/search?q=529+TECH+LLC+official+website 200\n",
      "2025-11-17 19:20:55,544 - INFO - response: https://www.bing.com/search?q=529+TECH+LLC+company 200\n",
      "2025-11-17 19:20:56,861 - INFO - response: https://www.bing.com/search?q=%22529+TECH+LLC%22 200\n",
      "2025-11-17 19:20:58,074 - INFO - response: https://www.bing.com/search?q=%22529+TECH+LLC%22&first=11&FORM=PERE 200\n",
      "2025-11-17 19:20:59,498 - INFO - response: https://www.bing.com/search?q=%22529+TECH+LLC%22&first=21&FORM=PERE1 200\n",
      "2025-11-17 19:21:00,805 - INFO - response: https://www.bing.com/search?q=%22529+TECH+LLC%22&first=31&FORM=PERE2 200\n",
      "2025-11-17 19:21:02,461 - INFO - response: https://www.bing.com/search?q=%22529+TECH+LLC%22&first=41&FORM=PERE3 200\n",
      "d:\\urlgenerator\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "2025-11-17 19:21:05,394 - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n"
     ]
    },
    {
     "ename": "NotFound",
     "evalue": "404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFound\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m      1\u001b[39m batch_input_noaddress = [\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcompany_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2ClickFit, Inc.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     }\n\u001b[32m     29\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m res = \u001b[43mresolve_super7_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_input_noaddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(res, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 981\u001b[39m, in \u001b[36mresolve_super7_batch\u001b[39m\u001b[34m(super7_payloads)\u001b[39m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m payload \u001b[38;5;129;01min\u001b[39;00m super7_payloads:\n\u001b[32m    980\u001b[39m     s7 = Super7Input(**payload)\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m     out = \u001b[43mresolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_company\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms7\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m     results.append(out)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m: results}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 914\u001b[39m, in \u001b[36mSuper7Resolver.process_company\u001b[39m\u001b[34m(self, s7)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m snippets:\n\u001b[32m    912\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m extraction = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_from_snippets\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms7\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnippets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    916\u001b[39m candidate_records.append(\n\u001b[32m    917\u001b[39m     CandidateRecord(\n\u001b[32m    918\u001b[39m         url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    921\u001b[39m     )\n\u001b[32m    922\u001b[39m )\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extraction.overall_score > primary_conf:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 629\u001b[39m, in \u001b[36mLLMExtractor.extract_from_snippets\u001b[39m\u001b[34m(self, super7, url, snippets)\u001b[39m\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PageExtractionResult(\n\u001b[32m    618\u001b[39m         url=url,\n\u001b[32m    619\u001b[39m         entities=[],\n\u001b[32m   (...)\u001b[39m\u001b[32m    625\u001b[39m         reason=\u001b[33m\"\u001b[39m\u001b[33mNo snippets extracted.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    626\u001b[39m     )\n\u001b[32m    628\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.build_extraction_prompt(super7, url, snippets)\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m text = response.content\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# Try to parse JSON\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:961\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    936\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    937\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    948\u001b[39m     **kwargs: Any,\n\u001b[32m    949\u001b[39m ) -> ChatResult:\n\u001b[32m    950\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    951\u001b[39m         messages,\n\u001b[32m    952\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m         tool_choice=tool_choice,\n\u001b[32m    960\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:194\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    191\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    192\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\urlgenerator\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mNotFound\u001b[39m: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_input_noaddress = [\n",
    "    {\n",
    "        \"company_name\": \"2ClickFit, Inc.\"\n",
    "    },\n",
    "    {\n",
    "        \"company_name\": \"529 TECH LLC\"\n",
    "    },\n",
    "    {\n",
    "        \"company_name\": \"901 Tax Pros LLC\"\n",
    "    },\n",
    "    {\n",
    "        \"company_name\": \"A2 Exteriors LLC\"\n",
    "    },\n",
    "    {\n",
    "        \"company_name\": \"AAF Logistics LLC\"\n",
    "    },\n",
    "    {\n",
    "        \"company_name\": \"Able Path Care & Staffing LLC\"\n",
    "    },\n",
    "    {\n",
    "        \"company_name\": \"Acirdek Solutions LLC\"\n",
    "    },\n",
    "    {\n",
    "        \"company_name\": \"Adams Site Works, LLC\"\n",
    "    },\n",
    "    {\n",
    "        \"company_name\": \"Adonai's Touch Cleaning LLC\"\n",
    "    }\n",
    "]\n",
    "\n",
    "res = resolve_super7_batch(batch_input_noaddress)\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e194a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
